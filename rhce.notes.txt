Features provided by systemd:

•  Parallelization capabilities, which increase the boot speed of a  system. 
•  On-demand starting of daemons without requiring a  separate service. 
•  Automatic service dependency management prevents  long  timeouts, such as not starting a network service when the network  is  not available. 
•  A method of tracking related processes together using Linux control  groups. 



 
[root@serverx  -]#  systemctl  status  sshd.service 
'  sshd.service  - OpenSSH  server  daemon 
	Loaded :  loaded  (/usr/lib/systemd/system/sshd.service;  enabled ) 
	Active :  active  (running )  since  Thu  2014-02-27  11:51:39  EST;  7h  ago 
	Main  PID : 1073 (sshd ) 
	CG group : /system.slice/sshd.service 
			   L. 1073  /usr/sbin/sshd  -D 
Feb  27  11 :51:39  server0.example.com  systemd [1] :  Started  OpenSSH  se rver  daemon . 
Feb  27  11 :51:39  server0.example.com  sshd [1073] :  Could  not  load  host  key :  /et  ...  y 
Feb  27  11 :51:39  server0.example.com  sshd [1073] :  Server  lis tening  on  0.0.0.0  ...  . 
Feb  27  11 :51:39  server0.example.com  sshd [1073] :  Server  listening  on  ::  port  22 . 
Feb  27  11 :53:21  server0.example.com  sshd [1270] :  error:  Could  not  load  host  k ...  y 
Feb  27  11 :53:22  server0.example.com  sshd [1270] :  Accepted  pas swo rd  for root  f ...  2 
Hint :  Some  lines  were  ellipsized ,  use  -1  to  show in  full . 

-------------------------------------------------------------------------------------------------
Keyword:  			|	Description: 
-------------------------------------------------------------------------------------------------
loaded  			|	Unit  conf iguration file has been processed. 
active (running)  	|	Running with one or more  continuing processes. 
active (exited)  	|	Successfully completed a  one-time configuration. 
active (waiting)  	|	Running but waiting for an event. 
inactive  			|	Not  running. 
enabled  			|	Will be started at boot time. 
disabled  			|	Will not be started at boot time. 
static  			|	Can not be enabled, but may be started by an enabled unit automatically. 
-------------------------------------------------------------------------------------------------


Query  the state of  all units to verify a  system startup. 
  [root@serverx  -]#  systemctl 

Query the state of only the service units. 
[ root@serverx  - ]#  systemctl -type=service 

Investigate any units which are  in a  failed or maintenance  state. Optionally,  add the  -1  option to show the full output. 
 [root@serverX  -]#  systemctl  status  rngd.service  -1 

The status argument may also be  used to  determine if a  particular unit is  active and show  if 
the unit  is enabled to start at boot time. Alternate commands can also easily show the active and enabled  states: 

[root@serverx  -]#  systemctl  is-enabled  sshd
[root@serverx  -]#  systemctl  is-active  sshd

View the enabled and disabled  settings for all units. Optionally,  limit  the type of  unit. 

systemctl  list-unit-files  --type=service

See only failed services：
systemctl  --failed  --type=service



[root@serverx  -]#  systemctl start sshd.service
[root@serverx  -]#  systemctl stop sshd.service

[root@serverx  -]#  systemctl restart sshd.service
[root@serverx  -]#  systemctl staus sshd.service

[root@serverx  -]#  systemctl reload sshd.service


Unit dependencies 
Services may be  started  as  dependencies of other services.  If a socket unit is enabled  and the 
service unit with the same name is not, the service will automatically  be started when a  request 
is made on the network socket. Services may also  be triggered by path units when a  file system 
condition is met. 

The systemctl  list-dependencies  UNIT 

command can be used to display a  tree of other 
units which must  be started in conjunction with a  specific unit. The  --reverse option to this 
command will show what units need to have the specified unit started in order to run.

 
Masking services 
A system may have conflicting services installed for a certain function, such as firewalls (iptables 
and firewalld). To  prevent an administrator from accidentally starting a  service, a  service may be 
masked. Masking creates a  link in the configuration directories so that if the service is started, 
nothing will happen.

[root@serverx  -]#  systemctl mask network
ln -s '/dev/null' '/etc/systemd/system/network.service'


---------------------------------------------------------------------------------------------------------------------
Command:  					        |  Task: 
---------------------------------------------------------------------------------------------------------------------
systemctl  status  UNIT  			|	View detailed information about a  unit state. 
systemctl  stop  UNIT  	 			|	Stop a  service on a  running  system. 
systemctl  start  UNIT   			|	Start a  service on a  running system. 
systemctl  restart  UNIT  			|	Restart a  service on a  running system. 
systemctl  reload  UNIT  			|	Reload configuration file of a  running service. 
systemctl  mask  UNIT    			|	Completely disable a  service from being started, both manually and at boot. 
systemctl  unmask  UNIT  			|	Make a masked service available. 
systemctl  enable  UNIT  			|	Configure a  service to  start at  boot time. 
systemctl  disable  UNIT 			|	Disable a  service from starting at  boot time. 
systemctl  list-dependencies UNIT	| List units which are  required and wanted by the specified unit. 
---------------------------------------------------------------------------------------------------------------------


A systemd target  is a set of systemd units that should be started to reach a  desired state. 
Important targets are  listed  in the  following table. 

-----------------------------------------------------------------------------------
Target  			|	Purpose 
-----------------------------------------------------------------------------------
graphical.target  	|	System supports multiple users, graphical and text-based logins. 
multi-user.target  	|	System supports multiple users, text-based  logins only. 
rescue.target  		|	sulogin prompt. basic system  initialization completed. 
emergency.target  	|	sulogin prompt. initramfs pivot complete and system root mounted on / read-only. 
-----------------------------------------------------------------------------------

[root@serverX  -]#  systemctl  list-units  --type=target  --all

On a running system, administrators  can choose to switch to a  different target using the 
systemctl  isolate command; for example,  

	systemctl  isolate multi-user.target 
	
Selecting a  different target at  boot time
 
To select a  different target at boot time, a  special option can be appended to the kernel 
command line from the boot loader: systemd.unit=. For example,  to boot  the system  into a 
rescue shell, pass the following option at  the interactive boot  loader menu: 

systemd.unit=rescue.target 


To  use this method of selecting a  different target, use the following procedure for Red Hat 
Enterprise Linux 7  systems: 
1.  (Re)boot the system. 
2.  Interrupt the boot  loader menu  countdown by pressing any key. 
3.  Move the cursor to  the entry to be started. 
4.  Press e to edit  the current entry. 
5.  Move  the cursor to the  line that starts with linux16. This  is the kernel command li ne. 
6.  Append systemd.unit=desired.target. 
7.  Press Ctrl +x  to  boot with these changes.


--- reset lost root password:

On Red Hat Enterprise Linux 7,  it is  possible to  have the scripts that run from the initramfs 
pause at certain points, provide a  root shell, and then continue when that shell exits. While this 
is mostly meant  for debugging,  it can also be used to recover a  lost root password: 
1.  Reboot the system. 
2.  Interrupt  the boot  loader countdown by pressing any  key. 
3.  Move the cursor to the entry  that needs to be booted. 
4.  Press e to edit the selected entry. 
5.  Move the cursor to the kernel command line (the  line that  starts with linux16. 
6.  Append rd.break (this will break just  before control  is handed  from the initramfs to the actual system). 
7.  Press Ctrl +x  to  boot with the changes.
 
At this point, a  root  shell will be presented, with the  root file system for the actual system mounted read-only on /sysroot.

To recover  the root password from this point. use the following procedure: 
1.  Remount /sysroot as read-write 
		mount  -oremount,rw  /sysroot
2. Switch  into a  chroot jail, where /sysroot  is treated  as the  root of the file system tree. 
		chroot /sysroot
3. Set a new root password: 
		passwd root
4. Make sure that all unlabeled files  (including /etc/shadow at this point) get relabeled during boot.
		touch  /.autorelabel 
5. Type exit twice. The first will exit the chroot jail, and the second will exit the initramfs debug shell. 

Diag nose and repair systemd boot  issues 

If there are problems during the starting of services, there are a few tools available to system 
administrators that can help with debugging and/or troubleshooting: 

Early debug shell 
By running systemctl  enable  debug-shell.service, a  root shell will be spawned on 
TTV9  (Ctrl+Alt+F9) early during the boot  sequence. This shell is automatically logged in as 
root so that an administrator can use some of the other debugging tools while the system  is still booting. 

Emergency and  rescue ta rgets 
By appending either systemd.unit=rescue.target or systemd.unit=emergency.target to  the  kernel command 
line from the boot loader, the system will spawn into a special rescue or emergency shell instead of starting normally.  Both of these shells require the root password. The emergency target keeps the root file system 
mounted read-only, while rescue.target waits for sysinit.target to complete first so that more of the system will be 
initialized, for example,  logging, file systems, etc. Exiting from these shells will continue with the  regular boot process.

Stuck  jobs 
During startup, systemd spawns a number of jobs.  If some of these jobs can not complete, they 
will block other jobs from running. To inspect the current job list. an administrator can use the 
command systemctl list-jobs. Any  jobs listed as running must complete before the jobs 
listed as waiting can continue. 







In  Red Hat Enterprise Linux 7, the configuration of network  interfaces is managed by a  system 
daemon called NetworkManager.  For  NetworkManager: 
•  A device  is a  network interface. 
•  A connection  is a  collection of settings that can be configured for a  device. 
•  Only one connection is active  for any one device at a  time. Multiple connections may exist, for 
use by different devices or to allow a  configuration to be altered for the same device. 
•  Each connection has a name or  ID  that identifies it. 
•  The persistent conf iguration for a  connection is stored  in 
/etc/sysconfig/network- scripts/ifcfg - name, where name  is the name of the 
con nection (although spaces are  normally  replaced with underscores in the file name). This file 
can  be edited by hand if desired. 
•  The nmcli utility can be used to create and edit connection files  from the shell prompt. 



------------------------------------------------------------------------------------------------------------------
Command  			Purpose
------------------------------------------------------------------------------------------------------------------
nmcli dev status		Show the NetworkManager status of all network interfaces. 
nmcli con show  		List  all connections. 
nmcli con show name  		List  the current settings for  the connection name. 
nmcli con add con-name name 	Add a  new connection named name. 
nmcli con  mod name ...  	Modify the connection name. 
nmcli con  reload  		Tell NetworkManager to reread the configuration files
nmcli con  up  name		Activate the connection name. 
nmcli dev  dis  dev  		Deactivate and disconnect the cu rrent connection on the network interface dev. 
nmcli con  del  name		Delete the connection name and its configuration file. 
ip  addr  show			Show  the current network  interface address configuration. 
hostnamectl set-hostname	Persistently set the host name on this system. 
------------------------------------------------------------------------------------------------------------------


Network teaming
Network teaming is method for linking NICs together logically to allow for failover or  higher 
throughput. Teaming is a  new implementation that does not affect the older bonding driver in 
the Linux kernel; it offers an alternate implementation. Red Hat Enterprise  Linux 7  supports 
channel bonding for  backward compatability. Network teaming provides better  performance and 
is more extensible because of its modular design. 
Red Hat Enterprise Linux 7  implements network teaming with a  small kernel driver and a  user­
space daemon, teamd. The kernel handles network packets efficiently and teamd handles logic 
and interface processing. Software, called runners,  implement load balancing and active-backup 
logic, such as round robin. The following runners are available to teamd:

· broadcast: a simple runner which transmits each packet from all ports. 
. round robin: a simple runner which transmits packets in a round-robin fashing from each of the ports. 
· activebackup: this is a failover runner which watches for link changes and  selects an active port for data transfers. 
· loadbalance: this runner monitors  traffic and uses a hash function to try to reach a  perfect 
balance when selecting ports for packet transmission. 
· lacp: implements the 802.3ad Link Aggregation Control Protocol. Can use the same transmit port selection possibilities as the loadbalance runner.



All network interaction is done through a team interface, composed of multiple network port 
interfaces. When controlling teamed port interfaces using NetworkManager,  and especially when 
fault finding, keep the following in mind: 
•  Starting the network team interface does not automatically start the port interfaces. 
•  Starting a port interface always starts the teamed interface. 
•  Stopping the teamed interface also stops the port interfaces. 
•  A teamed interface without ports can start static IP connections. 
•  A team without ports waits for ports when starting DHCP connections. 
•  A team with a DHCP connection waiting for ports completes when a port with a carrier is added. 
•  A team with a DHCP connection waiting for ports continues waiting when a port without a carrier is added. 

Configuring network teams 
The nmcli command can be used to create and manage team and port interfaces. The following 
four steps are used to create and activate a network team interface: 
1.  Create the team  interface. 
2.  Determine the  1Pv4 and/or 1Pv6 attributes of  the team  interface. 
3.  Assign the port interfaces. 
4.  Bring the team  and port interfaces up/down. 

Create the team interface 

Use  the nmcli command to create a connection for the network team  interface, with the 
following syntax: 

		nmcli con add type team con-name CNAME ifname INAME [config  JSON] 
		
where CNAME will be  the name used to refer to  the connection, INAME will be  the interface name, 
and JSON specifies the runner to be  used. JSON has the following syntax:

 
	'{ "runner":  {"name": "METHOD"}} ' 
	
where METHOD  is one of  the following: 
	broadcast, roundrobin, activebackup, loadbalance, or lacp.
	
[root@demo -)#  nmcli con add type team con-name team0 ifname team0 config  '{"runner": {"name": "loadbalance"}}'

Determine the IPv4/IPv6 attributes of  the team interface 
Once  the network team  interface is created, IPv4 and/or IPv6 attributes can be assigned to it. If 
DHCP is available, this step is optional, because the default attributes configure the interface to 
get its IP settings using DHCP

The following example demonstrates how to assign a static  IPv4 address  to  the team0  interface:

[root@demo -]# nmcli con  mod  team0  ipv4.addresses  1.2.3.4/24  
[root@demo -]# nmcli con  mod  team0  ipv4.method  manual


Assign ports to team interface:
	nmcli con add type team-slave  con-name CNAME ifname INAME master TEAM
	
When  the team  interface is up, the teamdctl command can be used to display the team's state. 
The output of this command includes the status of the port interfaces.


NetworkManager creates configuration files  for  network teaming in the 
/etc/sysconfig/network-scripts the same way it does for other interfaces. A 
configuration file is created for each of the interfaces: for the team, and each of the ports. 
The configuration file for the team  interface defines the IP settings for the interface. The 
DEVICETYPE variable informs the  initialization scripts this is a network team interface. 
Parameters for teamd configuration are defined  in the TEAM_CONFIG variable. Note that the 
contents of TEAM_CONFIG uses JSON syntax.

		# /etc/sysconfig/network-scripts/ifcfg-team0 
		DEVICE=team0 
		DEVICETYPE=Team 
		TEAM_CONFIG="{\"runner\":  {\"name\": \"broadcast\"}}" 
		BOOTPROTO=none 
		IPADDR0=172 .25.5.100 
		PREFIX0=24 
		NAME=team0
		ONBOOT=yes  
		
		# /etc/sysconfig/network-scripts/ifcfg-team0-eth1 
		DEVICE=eth1 
		DEVICETYPE=TeamPort 
		TEAM_MASTER=team0 
		NAME=team0-eth1 
		ONBOOT=yes
		

Initial  network team  configuration is set when the team  interface is created. The default  runner 
is roundrobin, but a different runner can be chosen by specifying a JSON string when the team 
is created with the team.config subcommand. Default values for runner parameters are  used 
when they are not specified.

		nmcli con mod IFACE team.config  JSON-configuration-file-or-string
		
# cat /tmp/team.conf	
{ 
	"device": "team0", 
	"mcast_rejoin": { 
		"count": 1 
	}, 
	"notify_peers" : { 
		"count": 1 
	}, 
	"ports": { 
		"eth1": { 
	"prio": -10, 
	"sticky": true, 
		"link_watch": { 
			"name": "ethtool" 
			} 
		},
		"eth2": { 
	"prio": 100, 
		"link_watch": { 
			"name": "ethtool"
				}
			}
		},
		"runner": { 
			"name": "activebackup"
			}
	}
[root@demo -]# nmcli con mod team0 team.config  /tmp/team.conf 



The link_watch settings in the configuration file determines how the  link state of the port 
interfaces are monitored. The default  looks like  the following, and uses functionality similar to 
the ethtool command to check the  link of each interface: 

	"link_watch": { 
		"name": "ethtool"
		}

Another way to  check link state  is to  periodically use an ARP ping packet to  check for remote 
connectivity.  Local  and remote  IP addresses and timeouts would have to be specified. A 
configuration that would accomplish that would look similar to  the following:

	"link_watch":{ 
		"name": "arp_ping", 
		"interval":  100, 
		"missed_max": 30, 
		"source_host": "192.168.23.2", 
		"target_host": "192.168.23.1"
	}, 
	
	
	
Display  the team  ports of the team0  interface:	
[root@demo -]# teamnl  team0  ports
4: eth2:  up 0Mbit HD 
3: eth1:  up 0Mbit HD 

Display the currently active port of team0:
[root@demo -]# teamnl  team0  getoption  activeport 
3 

Set the option for the active port of team0:
[root@demo -]# teamnl team0 setoption activeport 3

Use teamdctl to display the cu rrent state of the teama  interface: 

		[root@demo -]# teamdctl team0  state 
		setup: 
			runner:  activebackup 
		ports : 
			eth2 
				link watches: 
					link summary: up 
					instance[link_watch_0] : 
					name: ethtool 
					link: up 
			eth1
				link watches: 
					link summary: up 
					instance[link_watch_0] : 
						name: ethtool 
						link: up 
		runner: 
			active  port: eth1
			
			
teamdctl team0 config dump



Software bridges 

A network bridge is a link-layer device that forwards traffic between networks based on MAC 
addresses.  It learns what hosts are connected to  each network, builds a table of MAC addresses, 
then makes packet forwarding decisions based on that table. A  software bridge can be used in 
a  Linux environment to emulate a  hardware  bridge. The most common application for software 
bridges is in virtualization applications for sharing a hardware NIC among one or more virtual 
NICs. 

Configure  software  bridges 
The nmcli can be used to configure software bridges persistently.  First. the software bridge is 
created, then existing interfaces are  connected to it. For example,  the following commands will 
create a  bridge called br0  and attach both the ethl and eth2 interfaces to  it. 


[root@demo  -]#  nmcli  con add type bridge con-name  br0  ifname  br0 
[root@demo  -]#  nmcli  con add type bridge-slave  con-name  br0-port1  ifname  eth1  master  br0
[root@demo  -]#  nmcli  con add type bridge-slave  con-name  br0-port2  ifname  eth2  master  br0


Softwa re bridge configuration files 
Software bridges are managed by interface configuration files found in the 
/etc/sysconfig/network-scripts directory. There  is an ifcfg-*  configuration file for 
each software bridge. 
The following  is a sample configuration file for a  software bridge: 

		# /etc/sysconfig/network-scripts/ifcfg-br1
		DEVICE=br1 
		NAME=br1 
		TYPE=Bridge 
		BOOTPROTO=none 
		IPADDR0=172.25.5.100 
		PREFIX0=24 
		STP=yes 
		BRIDGING�OPTS=priority=32768
		
The following configuration file attaches an Ethernet  interface to a software bridge: 	
		# /etc/sysconfig/network-scripts/ifcfg-br1-port0
		TYPE=Ethernet 
		NAME=br1-port0 
		DEVICE=eth1 
		ONBOOT=yes 
		BRIDGE=br1
		
=====================  NOTE =====================	
To implement a  software bridge on  an existing teamed or  bonded network interface 
managed by NetworkManager,  NetworkManager wi ll have to be disabled since it only 
supports bridges on simple Ethernet  interfaces. Configuration files for  the bridge will 
have  to be created by hand. The ifup and ifdown utilities can be used to manage the 
software bridge and other network interfaces. 


The brctl show command will display  software bridges and the  list of  interfaces attached to 
them. 

[root@demo  -]#  brctl  show 
bridge name		bridge id 				STP enabled		interfaces
br1  			8000.52540001050b		yes				eth1
