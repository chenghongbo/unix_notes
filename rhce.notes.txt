Features provided by systemd:

•  Parallelization capabilities, which increase the boot speed of a  system. 
•  On-demand starting of daemons without requiring a  separate service. 
•  Automatic service dependency management prevents  long  timeouts, such as not starting a network service when the network  is  not available. 
•  A method of tracking related processes together using Linux control  groups. 



 
[root@serverx  -]#  systemctl  status  sshd.service 
'  sshd.service  - OpenSSH  server  daemon 
	Loaded :  loaded  (/usr/lib/systemd/system/sshd.service;  enabled ) 
	Active :  active  (running )  since  Thu  2014-02-27  11:51:39  EST;  7h  ago 
	Main  PID : 1073 (sshd ) 
	CG group : /system.slice/sshd.service 
			   L. 1073  /usr/sbin/sshd  -D 
Feb  27  11 :51:39  server0.example.com  systemd [1] :  Started  OpenSSH  se rver  daemon . 
Feb  27  11 :51:39  server0.example.com  sshd [1073] :  Could  not  load  host  key :  /et  ...  y 
Feb  27  11 :51:39  server0.example.com  sshd [1073] :  Server  lis tening  on  0.0.0.0  ...  . 
Feb  27  11 :51:39  server0.example.com  sshd [1073] :  Server  listening  on  ::  port  22 . 
Feb  27  11 :53:21  server0.example.com  sshd [1270] :  error:  Could  not  load  host  k ...  y 
Feb  27  11 :53:22  server0.example.com  sshd [1270] :  Accepted  pas swo rd  for root  f ...  2 
Hint :  Some  lines  were  ellipsized ,  use  -1  to  show in  full . 

-------------------------------------------------------------------------------------------------
Keyword:  		|	Description: 
-------------------------------------------------------------------------------------------------
loaded  		|	Unit  conf iguration file has been processed. 
active (running)  	|	Running with one or more  continuing processes. 
active (exited)  	|	Successfully completed a  one-time configuration. 
active (waiting)  	|	Running but waiting for an event. 
inactive  		|	Not  running. 
enabled  		|	Will be started at boot time. 
disabled  		|	Will not be started at boot time. 
static  		|	Can not be enabled, but may be started by an enabled unit automatically. 
-------------------------------------------------------------------------------------------------


Query  the state of  all units to verify a  system startup. 
  [root@serverx  -]#  systemctl 

Query the state of only the service units. 
[ root@serverx  - ]#  systemctl -type=service 

Investigate any units which are  in a  failed or maintenance  state. Optionally,  add the  -1  option to show the full output. 
 [root@serverX  -]#  systemctl  status  rngd.service  -1 

The status argument may also be  used to  determine if a  particular unit is  active and show  if 
the unit  is enabled to start at boot time. Alternate commands can also easily show the active and enabled  states: 

[root@serverx  -]#  systemctl  is-enabled  sshd
[root@serverx  -]#  systemctl  is-active  sshd

View the enabled and disabled  settings for all units. Optionally,  limit  the type of  unit. 

systemctl  list-unit-files  --type=service

See only failed services：
systemctl  --failed  --type=service



[root@serverx  -]#  systemctl start sshd.service
[root@serverx  -]#  systemctl stop sshd.service

[root@serverx  -]#  systemctl restart sshd.service
[root@serverx  -]#  systemctl staus sshd.service

[root@serverx  -]#  systemctl reload sshd.service


Unit dependencies 
Services may be  started  as  dependencies of other services.  If a socket unit is enabled  and the 
service unit with the same name is not, the service will automatically  be started when a  request 
is made on the network socket. Services may also  be triggered by path units when a  file system 
condition is met. 

The systemctl  list-dependencies  UNIT 

command can be used to display a  tree of other 
units which must  be started in conjunction with a  specific unit. The  --reverse option to this 
command will show what units need to have the specified unit started in order to run.

 
Masking services 
A system may have conflicting services installed for a certain function, such as firewalls (iptables 
and firewalld). To  prevent an administrator from accidentally starting a  service, a  service may be 
masked. Masking creates a  link in the configuration directories so that if the service is started, 
nothing will happen.

[root@serverx  -]#  systemctl mask network
ln -s '/dev/null' '/etc/systemd/system/network.service'


---------------------------------------------------------------------------------------------------------------------
Command:  					        |  Task: 
---------------------------------------------------------------------------------------------------------------------
systemctl  status  UNIT  			|	View detailed information about a  unit state. 
systemctl  stop  UNIT  	 			|	Stop a  service on a  running  system. 
systemctl  start  UNIT   			|	Start a  service on a  running system. 
systemctl  restart  UNIT  			|	Restart a  service on a  running system. 
systemctl  reload  UNIT  			|	Reload configuration file of a  running service. 
systemctl  mask  UNIT    			|	Completely disable a  service from being started, both manually and at boot. 
systemctl  unmask  UNIT  			|	Make a masked service available. 
systemctl  enable  UNIT  			|	Configure a  service to  start at  boot time. 
systemctl  disable  UNIT 			|	Disable a  service from starting at  boot time. 
systemctl  list-dependencies UNIT	| List units which are  required and wanted by the specified unit. 
---------------------------------------------------------------------------------------------------------------------


A systemd target  is a set of systemd units that should be started to reach a  desired state. 
Important targets are  listed  in the  following table. 

-----------------------------------------------------------------------------------
Target  			|	Purpose 
-----------------------------------------------------------------------------------
graphical.target  	|	System supports multiple users, graphical and text-based logins. 
multi-user.target  	|	System supports multiple users, text-based  logins only. 
rescue.target  		|	sulogin prompt. basic system  initialization completed. 
emergency.target  	|	sulogin prompt. initramfs pivot complete and system root mounted on / read-only. 
-----------------------------------------------------------------------------------

[root@serverX  -]#  systemctl  list-units  --type=target  --all

On a running system, administrators  can choose to switch to a  different target using the 
systemctl  isolate command; for example,  

	systemctl  isolate multi-user.target 
	
Selecting a  different target at  boot time
 
To select a  different target at boot time, a  special option can be appended to the kernel 
command line from the boot loader: systemd.unit=. For example,  to boot  the system  into a 
rescue shell, pass the following option at  the interactive boot  loader menu: 

systemd.unit=rescue.target 


To  use this method of selecting a  different target, use the following procedure for Red Hat 
Enterprise Linux 7  systems: 
1.  (Re)boot the system. 
2.  Interrupt the boot  loader menu  countdown by pressing any key. 
3.  Move the cursor to  the entry to be started. 
4.  Press e to edit  the current entry. 
5.  Move  the cursor to the  line that starts with linux16. This  is the kernel command li ne. 
6.  Append systemd.unit=desired.target. 
7.  Press Ctrl +x  to  boot with these changes.


--- reset lost root password:

On Red Hat Enterprise Linux 7,  it is  possible to  have the scripts that run from the initramfs 
pause at certain points, provide a  root shell, and then continue when that shell exits. While this 
is mostly meant  for debugging,  it can also be used to recover a  lost root password: 
1.  Reboot the system. 
2.  Interrupt  the boot  loader countdown by pressing any  key. 
3.  Move the cursor to the entry  that needs to be booted. 
4.  Press e to edit the selected entry. 
5.  Move the cursor to the kernel command line (the  line that  starts with linux16. 
6.  Append rd.break (this will break just  before control  is handed  from the initramfs to the actual system). 
7.  Press Ctrl +x  to  boot with the changes.
 
At this point, a  root  shell will be presented, with the  root file system for the actual system mounted read-only on /sysroot.

To recover  the root password from this point. use the following procedure: 
1.  Remount /sysroot as read-write 
		mount  -oremount,rw  /sysroot
2. Switch  into a  chroot jail, where /sysroot  is treated  as the  root of the file system tree. 
		chroot /sysroot
3. Set a new root password: 
		passwd root
4. Make sure that all unlabeled files  (including /etc/shadow at this point) get relabeled during boot.
		touch  /.autorelabel 
5. Type exit twice. The first will exit the chroot jail, and the second will exit the initramfs debug shell. 

Diag nose and repair systemd boot  issues 

If there are problems during the starting of services, there are a few tools available to system 
administrators that can help with debugging and/or troubleshooting: 

Early debug shell 
By running systemctl  enable  debug-shell.service, a  root shell will be spawned on 
TTV9  (Ctrl+Alt+F9) early during the boot  sequence. This shell is automatically logged in as 
root so that an administrator can use some of the other debugging tools while the system  is still booting. 

Emergency and  rescue ta rgets 
By appending either systemd.unit=rescue.target or systemd.unit=emergency.target to  the  kernel command 
line from the boot loader, the system will spawn into a special rescue or emergency shell instead of starting normally.  Both of these shells require the root password. The emergency target keeps the root file system 
mounted read-only, while rescue.target waits for sysinit.target to complete first so that more of the system will be 
initialized, for example,  logging, file systems, etc. Exiting from these shells will continue with the  regular boot process.

Stuck  jobs 
During startup, systemd spawns a number of jobs.  If some of these jobs can not complete, they 
will block other jobs from running. To inspect the current job list. an administrator can use the 
command systemctl list-jobs. Any  jobs listed as running must complete before the jobs 
listed as waiting can continue. 







In  Red Hat Enterprise Linux 7, the configuration of network  interfaces is managed by a  system 
daemon called NetworkManager.  For  NetworkManager: 
•  A device  is a  network interface. 
•  A connection  is a  collection of settings that can be configured for a  device. 
•  Only one connection is active  for any one device at a  time. Multiple connections may exist, for 
use by different devices or to allow a  configuration to be altered for the same device. 
•  Each connection has a name or  ID  that identifies it. 
•  The persistent configuration for a  connection is stored  in 
/etc/sysconfig/network-scripts/ifcfg-name, where name  is the name of the 
connection (although spaces are  normally  replaced with underscores in the file name). This file 
can  be edited by hand if desired. 
•  The nmcli utility can be used to create and edit connection files  from the shell prompt. 



------------------------------------------------------------------------------------------------------------------
Command  			Purpose
------------------------------------------------------------------------------------------------------------------
nmcli dev status		Show the NetworkManager status of all network interfaces. 
nmcli con show  		List  all connections. 
nmcli con show name  		List  the current settings for  the connection name. 
nmcli con add con-name name 	Add a  new connection named name. 
nmcli con  mod name ...  	Modify the connection name. 
nmcli con  reload  		Tell NetworkManager to reread the configuration files
nmcli con  up  name		Activate the connection name. 
nmcli dev  dis  dev  		Deactivate and disconnect the cu rrent connection on the network interface dev. 
nmcli con  del  name		Delete the connection name and its configuration file. 
ip  addr  show			Show  the current network  interface address configuration. 
hostnamectl set-hostname	Persistently set the host name on this system. 
------------------------------------------------------------------------------------------------------------------


Network teaming
Network teaming is method for linking NICs together logically to allow for failover or  higher 
throughput. Teaming is a  new implementation that does not affect the older bonding driver in 
the Linux kernel; it offers an alternate implementation. Red Hat Enterprise  Linux 7  supports 
channel bonding for  backward compatability. Network teaming provides better  performance and 
is more extensible because of its modular design. 
Red Hat Enterprise Linux 7  implements network teaming with a  small kernel driver and a  user­
space daemon, teamd. The kernel handles network packets efficiently and teamd handles logic 
and interface processing. Software, called runners,  implement load balancing and active-backup 
logic, such as round robin. The following runners are available to teamd:

· broadcast: a simple runner which transmits each packet from all ports. 
. round robin: a simple runner which transmits packets in a round-robin fashing from each of the ports. 
· activebackup: this is a failover runner which watches for link changes and  selects an active port for data transfers. 
· loadbalance: this runner monitors  traffic and uses a hash function to try to reach a  perfect 
balance when selecting ports for packet transmission. 
· lacp: implements the 802.3ad Link Aggregation Control Protocol. Can use the same transmit port selection possibilities as the loadbalance runner.



All network interaction is done through a team interface, composed of multiple network port 
interfaces. When controlling teamed port interfaces using NetworkManager,  and especially when 
fault finding, keep the following in mind: 
•  Starting the network team interface does not automatically start the port interfaces. 
•  Starting a port interface always starts the teamed interface. 
•  Stopping the teamed interface also stops the port interfaces. 
•  A teamed interface without ports can start static IP connections. 
•  A team without ports waits for ports when starting DHCP connections. 
•  A team with a DHCP connection waiting for ports completes when a port with a carrier is added. 
•  A team with a DHCP connection waiting for ports continues waiting when a port without a carrier is added. 

Configuring network teams 
The nmcli command can be used to create and manage team and port interfaces. The following 
four steps are used to create and activate a network team interface: 
1.  Create the team  interface. 
2.  Determine the  1Pv4 and/or 1Pv6 attributes of  the team  interface. 
3.  Assign the port interfaces. 
4.  Bring the team  and port interfaces up/down. 

Create the team interface 

Use  the nmcli command to create a connection for the network team  interface, with the 
following syntax: 

		nmcli con add type team con-name CNAME ifname INAME [config  JSON] 
		
where CNAME will be  the name used to refer to  the connection, INAME will be  the interface name, 
and JSON specifies the runner to be  used. JSON has the following syntax:

 
	'{ "runner":  {"name": "METHOD"}} ' 
	
where METHOD  is one of  the following: 
	broadcast, roundrobin, activebackup, loadbalance, or lacp.
	
[root@demo -)#  nmcli con add type team con-name team0 ifname team0 config  '{"runner": {"name": "loadbalance"}}'

Determine the IPv4/IPv6 attributes of  the team interface 
Once  the network team  interface is created, IPv4 and/or IPv6 attributes can be assigned to it. If 
DHCP is available, this step is optional, because the default attributes configure the interface to 
get its IP settings using DHCP

The following example demonstrates how to assign a static  IPv4 address  to  the team0  interface:

[root@demo -]# nmcli con  mod  team0  ipv4.addresses  1.2.3.4/24  
[root@demo -]# nmcli con  mod  team0  ipv4.method  manual


Assign ports to team interface:
	nmcli con add type team-slave  con-name CNAME ifname INAME master TEAM
	
When  the team  interface is up, the teamdctl command can be used to display the team's state. 
The output of this command includes the status of the port interfaces.


NetworkManager creates configuration files  for  network teaming in the 
/etc/sysconfig/network-scripts the same way it does for other interfaces. A 
configuration file is created for each of the interfaces: for the team, and each of the ports. 
The configuration file for the team  interface defines the IP settings for the interface. The 
DEVICETYPE variable informs the  initialization scripts this is a network team interface. 
Parameters for teamd configuration are defined  in the TEAM_CONFIG variable. Note that the 
contents of TEAM_CONFIG uses JSON syntax.

		# /etc/sysconfig/network-scripts/ifcfg-team0 
		DEVICE=team0 
		DEVICETYPE=Team 
		TEAM_CONFIG="{\"runner\":  {\"name\": \"broadcast\"}}" 
		BOOTPROTO=none 
		IPADDR0=172 .25.5.100 
		PREFIX0=24 
		NAME=team0
		ONBOOT=yes  
		
		# /etc/sysconfig/network-scripts/ifcfg-team0-eth1 
		DEVICE=eth1 
		DEVICETYPE=TeamPort 
		TEAM_MASTER=team0 
		NAME=team0-eth1 
		ONBOOT=yes
		

Initial  network team  configuration is set when the team  interface is created. The default  runner 
is roundrobin, but a different runner can be chosen by specifying a JSON string when the team 
is created with the team.config subcommand. Default values for runner parameters are  used 
when they are not specified.

		nmcli con mod IFACE team.config  JSON-configuration-file-or-string
		
# cat /tmp/team.conf	
{ 
	"device": "team0", 
	"mcast_rejoin": { 
		"count": 1 
	}, 
	"notify_peers" : { 
		"count": 1 
	}, 
	"ports": { 
		"eth1": { 
	"prio": -10, 
	"sticky": true, 
		"link_watch": { 
			"name": "ethtool" 
			} 
		},
		"eth2": { 
	"prio": 100, 
		"link_watch": { 
			"name": "ethtool"
				}
			}
		},
		"runner": { 
			"name": "activebackup"
			}
	}
[root@demo -]# nmcli con mod team0 team.config  /tmp/team.conf 



The link_watch settings in the configuration file determines how the  link state of the port 
interfaces are monitored. The default  looks like  the following, and uses functionality similar to 
the ethtool command to check the  link of each interface: 

	"link_watch": { 
		"name": "ethtool"
		}

Another way to  check link state  is to  periodically use an ARP ping packet to  check for remote 
connectivity.  Local  and remote  IP addresses and timeouts would have to be specified. A 
configuration that would accomplish that would look similar to  the following:

	"link_watch":{ 
		"name": "arp_ping", 
		"interval":  100, 
		"missed_max": 30, 
		"source_host": "192.168.23.2", 
		"target_host": "192.168.23.1"
	}, 
	
	
	
Display  the team  ports of the team0  interface:	
[root@demo -]# teamnl  team0  ports
4: eth2:  up 0Mbit HD 
3: eth1:  up 0Mbit HD 

Display the currently active port of team0:
[root@demo -]# teamnl  team0  getoption  activeport 
3 

Set the option for the active port of team0:
[root@demo -]# teamnl team0 setoption activeport 3

Use teamdctl to display the cu rrent state of the teama  interface: 

		[root@demo -]# teamdctl team0  state 
		setup: 
			runner:  activebackup 
		ports : 
			eth2 
				link watches: 
					link summary: up 
					instance[link_watch_0] : 
					name: ethtool 
					link: up 
			eth1
				link watches: 
					link summary: up 
					instance[link_watch_0] : 
						name: ethtool 
						link: up 
		runner: 
			active  port: eth1
			
			
teamdctl team0 config dump



Software bridges 

A network bridge is a link-layer device that forwards traffic between networks based on MAC 
addresses.  It learns what hosts are connected to  each network, builds a table of MAC addresses, 
then makes packet forwarding decisions based on that table. A  software bridge can be used in 
a  Linux environment to emulate a  hardware  bridge. The most common application for software 
bridges is in virtualization applications for sharing a hardware NIC among one or more virtual 
NICs. 

Configure  software  bridges 
The nmcli can be used to configure software bridges persistently.  First. the software bridge is 
created, then existing interfaces are  connected to it. For example,  the following commands will 
create a  bridge called br0  and attach both the ethl and eth2 interfaces to  it. 


[root@demo  -]#  nmcli  con add type bridge con-name  br0  ifname  br0 
[root@demo  -]#  nmcli  con add type bridge-slave  con-name  br0-port1  ifname  eth1  master  br0
[root@demo  -]#  nmcli  con add type bridge-slave  con-name  br0-port2  ifname  eth2  master  br0


Softwa re bridge configuration files 
Software bridges are managed by interface configuration files found in the 
/etc/sysconfig/network-scripts directory. There  is an ifcfg-*  configuration file for 
each software bridge. 
The following  is a sample configuration file for a  software bridge: 

		# /etc/sysconfig/network-scripts/ifcfg-br1
		DEVICE=br1 
		NAME=br1 
		TYPE=Bridge 
		BOOTPROTO=none 
		IPADDR0=172.25.5.100 
		PREFIX0=24 
		STP=yes 
		BRIDGING�OPTS=priority=32768
		
The following configuration file attaches an Ethernet  interface to a software bridge: 	
		# /etc/sysconfig/network-scripts/ifcfg-br1-port0
		TYPE=Ethernet 
		NAME=br1-port0 
		DEVICE=eth1 
		ONBOOT=yes 
		BRIDGE=br1
		
=====================  NOTE =====================	
To implement a  software bridge on  an existing teamed or  bonded network interface 
managed by NetworkManager,  NetworkManager wi ll have to be disabled since it only 
supports bridges on simple Ethernet  interfaces. Configuration files for  the bridge will 
have  to be created by hand. The ifup and ifdown utilities can be used to manage the 
software bridge and other network interfaces. 


The brctl show command will display  software bridges and the  list of  interfaces attached to 
them. 

[root@demo  -]#  brctl  show 
bridge name		bridge id 				STP enabled		interfaces
br1  			8000.52540001050b		yes				eth1

# Ping  the local network gateway using the software bridge. 
[root@serverX  -]#  ping  -I  brl  192.168.0.254 
PING  192 .168.0.254  (192.168.0.254 )  from  192 .168.0.100  brl :  56 (84)  bytes  of  data . 
64  bytes  from  192 .168.0.254 :  icmp_seq=10  ttl=64  time=0 .520  ms 
64  bytes  from  192 .168.0.254 :  icmp_seq=11  ttl=64  time=0 .470  ms 
64  bytes  from  192 .168.0.254 :  icmp_seq=12  ttl=64  time=0 .339  ms 
64  bytes  from  192 .168.0.254 :  icmp_seq=13  ttl=64  time=0 .294  ms 
...  Output  omitted  ... 



firewalld  is the default method in Red Hat Enterprise Linux 7  for managing host-level 
fi rewa lls. Sta rted from the firewalld.service systemd service, firewalld manages the 
Linux kernel netfilter subsystem  using the low-level iptables, ip6tables, and ebtables 
commands.

=================== NOTE ===============
The firewalld.service and iptables.service, ip6tables.service, and 
ebtables.service services conflict with each other. To  prevent accidentally starting 
one of the *tables.service services (and wiping out any running firewall config  in 
the process), it is good practice  to mask them using  systemctl. 

[root@serverX  -]#  for  SERVICE  in  iptables  ip6tables  ebtables ;  do 
>  systemctl  mask  ${SERVICE}.service 
>  done 


firewalld separates all incoming traffic  into zones, with each zone having  its own set of rules. 
To check which zone to use for an incoming connection, firewalld  uses this  logic, where the 
first  rule that matches wins: 

1.  If the source address of an  incoming packet matches a  source  rule setup for a  zone, that 
packet will  be routed through that zone. 
2.  If the incoming interface for a  packet matches a  filter setup for a zone, that zone will be 
used. 
3.  Otherwise, the default zone  is used. The default zone  is not a  separate zone;  instead,  it 
points to one of the other zones defined  on the system.
 
Unless overridden by an administrator or a  NetworkManager configuration, the default zone for 
any new network interface will be set to the public zone. 


Zone  name  		Default configuration 
---------------------------------------------------------------------------------------------------
trusted				Allow all incoming traffic. 
---------------------------------------------------------------------------------------------------	
home 				Reject incoming traffic unless  related to outgoing traffic or matching 
					the ssh, mdns, ipp-client, samba-client, or dhcpv6-client predefined services. 
---------------------------------------------------------------------------------------------------
internal  			Reject incoming traffic unless related to outgoing traffic or matching 
					the ssh, mdns, ipp-client, samba-client, or dhcpv6-client predefined services。
---------------------------------------------------------------------------------------------------					
work  				Reject  incoming traffic unless related to outgoing traffic or matching 
					the ssh, ipp-client, or dhcpv6-client predefined services.					
---------------------------------------------------------------------------------------------------
public  			Reject  incoming traffic unless  related to outgoing traffic or matching 
					the ssh or dhcpv6-client predefined services. The default zone for newly added 
					network interfaces.
---------------------------------------------------------------------------------------------------					
external  			Reject incoming traffic unless related to outgoing traffic or matching 
					the ssh predefined service. Outgoing  IPv4 traffic forwarded through 
					this zone  is masqueraded to  look like  it originated from the  IPv4 
					address of the outgoing network  interface. 
---------------------------------------------------------------------------------------------------					
dmz  				Reject incoming traffic unless related  to outgoing traffic or matching 
					the ssh predefined service. 
---------------------------------------------------------------------------------------------------					
block  				Reject all incoming traffic unless  related  to  outgoing traffic.
---------------------------------------------------------------------------------------------------					
drop  				Drop all  incoming traffic unless related to outgoing traffic (do not even 
					respond with  ICMP errors). 					
---------------------------------------------------------------------------------------------------	


Managing firewalld 
firewalld  can be managed  in three ways: 
1.  Using the command-line tool firewall-cmd. 
2.  Using the graphical tool firewall-config. 
3.  Using the configuration files  in /etc/firewalld/

In most cases, editing the configuration files directly is not  recommended, but  it can be useful to 
copy configurations in this way when using configuration management tools. 

Configure  firewall settings with firewall-cmd 

This section will focus on managing firewalld  using the command-line tool firewall- cmd. 
firewall-cmd  is installed as part of the main firewalld package. firewall-cmd  can  perform 
the same actions as firewall-config. 
The fol lowing table lists a  number of frequently used firewall-cmd commands, along with an 
explanation. Note that unless otherwise specified, almost all commands will work on the runtime 
configuration, unless the --permanent option is specified. Many of the commands listed take 
the --zone=<ZONE>  option to determine which zone  they affect.  If --zone is omitted from 
those commands, the default zone is used. 
While configuring a  firewall, an administrator will normally apply all changes to the  - -
permanent configuration, and then activate  those changes with firewall-cmd  --reload. 
While testing out new, and possibly dangerous, rules,  an administrator can choose to work 
on the runtime configuration by omitting the  --permanent  option. In those cases, an extra 
option can be  used to automatically remove a  rule after a  certain amount of time, preventing an 
administrator from accidentally locking out a  system:  - -timeout=<TIMEINSECONDS>。

fi rewa ll -cmd example 
The followi ng examples  show  the default  zone being set  to dmz, all traffic coming from the 
192.168.0.0/24 network being assigned to the in ternal zone, and the network ports for 
mysql being opened on the internal zone. 


[root@serverX  -]#  firewall-cmd --set-default --zone=dmz 
[root@serverx  -]#  firewall-cmd --permanent  --zone=internal  --add-source=192.168.0.0/24
[root@serverX  -]#  firewall-cmd --permanent  --zone=internal  --add-service=mysql 
[root@serverx  -]#  firewall-cmd --reload 

Rich rules  concepts 
Apart  from the regular zones and services syntax that firewalld offers, administrators  have 
two other options  for adding  firewall rules: direct rules  and rich rules. 

Direct  rules 
Direct rules allow an administrator to insert hand-coded {ip,ip6,eb}tables rules  into 
the zones managed by firewalld. While powerful, and exposing features of the kernel 
netfilter subsystem not exposed through other means, these  rules  can be hard  to manage. 
Direct rules  also offer less flexibility than standard  rules and rich rules. Configuring direct 
rules is not covered  in this course, but documentation is available in the firewall-cmd(1) 
and firewalld.direct (5) man pages for those administrators who are  already familiar with 
{ip,ip6,eb}tables syntax. 
Unless explicitly inserted  into a zone managed by firewalld, direct rules  will  be parsed before 
any firewalld rules are. 

[root@se rverx  -)# firewall-cmd  --direct  --permanent  --add-chain ipv4  raw  blacklist 
[root@serverX  -]#  firewall-cmd  --direct  --permanent  --add-rule  ipv4  raw  PREROUTING 0 -s \
192.168.0.0/24  -j  blacklist 
[root@serverx  -]#  firewall-cmd  --direct  --permanent  --add-rule  ipv4  raw  blacklist  0  -m \ 
limit  --limit  1/min  -j  LOG  -- log-prefix  "blacklisted " 
[root@serverX  -]#  firewall-cmd  --direct  --permanent  --add-rule  ipv4  raw  blacklist  1  -j DROP 

Rich rules 
firewalld rich  rules give administrators an expressive  language in which to express custom 
firewall rules that are  not covered by the basic firewalld syntax; for example,  to only allow 
connections to a  service from a  single IP address, instead of  all IP addresses routed through a zone. 
Rich rules  can be used to ex press basic all ow/deny  rules, but can also  be used to configure 
logging, both to  syslog and auditd, as  we ll as  port forwards, masquerading, and rate limiting. 

The basic syntax  of a  rich rule can  be expressed by the following block: 

	rule 
		[source] 
		[destination] 
		service | port | protocol | icmp-block | masquerade | forward-port 
		[log] 
		[audit] 
		[accept|reject|drop] 

Rule ordering 

Once multiple rules have been added to a zone  (or the firewall in genera l), the ordering of rules 
can have a  big effect on how  the firewall behaves. 
The basic ordering of rules  inside a  zone  is the same  for all zones: 

1.  Any port forwarding and masquerading rules set  for that zone. 
2.  Any  logging rules set for that zone. 
3.  Any allow rules set for that zone. 
4.  Any deny rules set  for that zone. 

In  all cases, the first match will win. If  a  packet has not been matched  by any rule  in  a  zone,  it will 
typically be denied, but zones might have a  different default; for example, the trusted zone will 
accept any  unmatched packet. Also, after matching a  logging rule, a  packet will continue to be 
processed as normal. 
Direct rules are an exception. Most direct rules will be parsed before any other processing is done 
by firewalld,  but the direct rule syntax allows an administrator to  insert any  rule they want 
anywhere in any zone. 

Testing and  debugging 
To make  testing and  debugging easier,  almost all  rules can be added to the runtime configuration 
with a  timeout. The moment the rule with a  timeout  is added to the firewall, the timer starts 
counting down for that  ru le. Once the timer for a  rule  has reached zero seconds, that  rule  is 
removed from the runtime configuration. 
Using timeouts can be an incredibly useful toot while working on a  remote  firewalls, especially 
when testing more complicated  rule  sets.  If a  rule works, the administrator can add  it  again, 
but with the --permanent option  (or at  least without a  timeout).  If the rule does not work 
as intended, maybe even  locking the administrator out of the system, it will be removed 
automatically, al lowing the administrator to continue  his or her work. 
A timeout  is added to a  runtime rule by adding the option  --timeout=<TIMEINSECONDS>  to 
the end of  the firewall-cmd  that enables  the rule

[root@serverX  -]# firewall-cmd  --permanent --zone=classroom  --add-rich-rule='rule family=ipv4  source address=192.168.0.11/32 reject' 

The difference  between reject and drop lies  in the fact that a  reject will send 
back an  ICMP packet detailing that, and why, a  connection was rejected. A drop  just 
drops the packet and does nothing else. Normally an administrator will want to use 
reject for friendly and neutral networks, and drop only for hostile networks


Logging with  rich rules 
When debugging, or monitoring, a  firewall, it  can be  useful to  have a  log of  accepted or  rejected 
connections. firewalld can accomplish this in two ways: by  logging to syslog, or by sending 
messages to the kernel audit subsystem, managed by auditd. 
In both cases, logging can be rate  limited. Rate limiting ensures that system  log files  do not fill up 
with messages at a  rate such that the system can not  keep up, or fills all its disk space. 
The basic syntax  for  logging to syslog using rich rules  is

log  [prefix="<PREFIX  TEXT>"  [level=<LOGLEVEL>] [limit value="<RATE/DURATION>"]

Where <LOGLEVEL>  is  one of emerg, alert, crit, error, warning, notice, info, or debug. 
<DURATION>  can be one of s for seconds, m for minutes, h  for hours, or d  for days. For example, 
limit  value=3/m will limit the log messages to a maximum of three per minute. 
The basic syntax  for logging to the audit subsystem is: 

audit  [limit  value= "<RATE/DURATION>"] 



Network Address Translation  (NAT) 
firewalld  supports two types of Network Address Translation  (NAT): masquerading and port 
forwarding.  Both can be configured on  a  basic level with regular firewall-cmd  rules, and more 
advanced forwarding configurations can be accomplished with rich rules. Both forms of  NAT 
modify  certain aspects of a  packet,  like the  source or  destination, before sending  it  on. 

Masquerading  ( IPV4 only)
With masquerading, a  system will forward packets that are  not directly addressed  to  itself to 
the  intended  recipient, while  changing the source address of the packets that  go through to  its 
own public  IP address. When answers to  those packets  come  in, the firewall will then modify  the 
destination address to the address of the original host, and send the  packet on. This is usually 
used on the edge of a  network to provide  Internet access to an internal network. Masquerading  is 
a  form of Network Address Translation  (NAT). 


To  configure masquerading for a zone with regular firewall-cmd commands, use the following syntax: 

[root@serverX  -]#  firewall-cmd  --permanent  --zone=<ZONE>  -- add-masquerade

This will masquerade any  packets sent to  the firewa ll from clients defi ned in the sou rces for that 
zone (both  inte rfaces and sub nets) that are  not  addressed to the firewall itself. 
To  gain more  control over what clients will be masqueraded, a  rich rule can be  used as  well. 

[root@serverX  -]#  firewall-cmd  --permanent  --zone=<ZONE>  --add-rich-rule='rule 
family=ipv4  source address=192.168.0.0/24  masquerade'



Port  forwarding 
Another form of NAT  is  port forwarding. With port forwarding, traffic to a  single port is  forwarded 
either to a  different port on the same machine, or to a  port on a  different machine. This 
mechanism is typically used to "hide" a  server  behind another machine, or to provide access to a 
service on  an alternate port. 

Important 
When a  port forward  is configured to forward packets to a  different machine, any 
replies from that machine will normally be  sent directly to  the original client  from that
machine. Since this will result in an invalid connection on most configurations,  the 
machine that is forwarded to will have  to be masqueraded through the firewa ll that 
performed the port  forwarding. 
A common configuration is to forward a  port  from the firewall machine to a machine 
that is already masqueraded behind the firewall. 

As an example,  the following command will forwa rd  incoming con nections on port 513/TCP on 
the fi rewa ll to port 132/TCP on the machine with the IP address 192  . 168 . 0.  254 for clients 
from the public zone:

firewall-cmd --permanent --zone=public  --add-forward-port=port=513:proto=tcp:toport=132:toaddr=192.168.0.254

To gain more  control over port forwarding rules, the following syntax can be  used with rich rules: 
forward-port  port=<PORTNUM>  protocol=tcp|udp  [to-port=<PORTNUM>]  [to-addr=<ADDRESS>]

SELinux port  labeling 
SELinux does more  than  just  file and process  labeling. Network traffic is  also tightly enforced 
by the SELinux policy. One of the methods that SELinux uses for controlling network traffic 
is labeling network ports; for example,  in the targeted policy,  port 22/TCP has the  label 
ssh_port_t associated with it. 
Whenever a  process wants to  listen on a port, SELinux will check to see if the label associated 
with that process (the domain) is allowed to bind that port  label. This can stop a  rogue service 
from taking over ports otherwise used by other (legitimate) network services.

[root@serverx  -]#  semanage  port  -1 
http_cache_port_t 	tcp  8080,  8118 ,  8123 ,  10001 - 10010 
http_cache_port_t 	udp  3130 
http_port_t 		tcp  80 ,  81,  443 ,  488 ,  8008 ,  8009 ,  8443 ,  9000 
......

For example,  to  allow a  gopher service to  listen on port 71/TCP:

semanage  port -a -t  gopher_port_t  -p  tcp  71

To remove the label:

semanage  port  -d -t  gopher_port_t  -p  tcp  71 

Introduction to  iSCSI 
The  Internet Small Computer System  Interface  (iSCSI)  is a TCP/IP-based protocol  for emulating 
a  SCSI high-performance  local storage bus over  IP networks, providing data transfer and 
management to  remote block storage devices. As a  storage area network (SAN) protocol, iSCSI 
extends SANs across  local  and wide area networks  (LANs, WANs, and the  Internet), providing 
location-independent data storage  retrieval with distributed servers and arrays. 

The SCSI protocol suite provides the Command Descriptor Block (COB)  command set  over a 
device bus communication protocol. The original SCSI topology used physical cabling with a 20-
meter limitation for all devices per channel (cabled  bus). Devices used unique numeric target 
IDs  (0-7, or  0-15 with dual channel). Physical SCSI disks and cabling were obsoleted by popular 
implementation of Fibre Channel (FC), which retained the SCSI COB command set but replaced 
the disk and  bus communication with protocols for  longer and faster optical cabling. 
The iSCSI protocol also  retains the COB command set, performing  bus communication between 
iSCSI systems that is encapsulated over standard  TCP/ IP.  iSCSI servers emulate SCSI  devices 
using files,  logical volumes, or disks of any  type as the  underlying storage (backstore)  presented 
as targets. An iSCSI service  is typically implemented  in  software above either an operating 
system TCP/IP stack or a  TCP offload engine (TOE), a  specialized Ethernet  network interface
The use of  iSCSI extends a  SAN beyond the limits of  local  cabling, facilitating storage 
consolidation in local or remote data centers. Because  iSCSI structures are logical, new storage 
allocations are made using only software configuration, without the need for additional cable 
or physical disks.  iSCS I  also simplifies data replication, migration and disaster recovery using 
multiple remote data centers

iSCSI fundamentals 
The iSCSI protocol functions in a  familiar client-server configuration. Client systems configure 
initiator software  to  send SCSI commands to  remote server storage targets. Accessed  iSCSI 
targets appear on the client  system as local, unformatted SCSI block devices,  identical  to devices 
connected with SCSI  cabling, FC direct attached, or FC switched fabric. 

Unlike  local block devices,  iSCSI network-accessed block devices are discoverable from many 
remote initiators. Typical  local file systems (e.g., ext4, XFS, btrfs) do not support concurrent 
multisystem mounting, which  can result  in  significant file system corruption. Clustered systems 
resolve multiple system access by use of the Global File System (GFS2), designed to provide 
distributed  file  locking and concurrent multinode file system mounting. 

iSCS I  target  overview 
In original SCS I  protocol  terminology, a  target  is a single connectible storage or output device 
uniquely identified on a  SCSI bus.  In iSCS I, in which the SCSI bus  is  emulated across an IP 
network, a  target ca n  be a  dedicated physical  device in a  network-attached storage enclosure or 
an iSCSI software-configured  logical device on a  networked storage server. A  target.  like  HBAs 
and  initiators, is an end point  in SCSI bus communication, passing command descriptor blocks 
(COB) to request or provide storage transactions. 
To  provide access to the storage or output device, a  target is configured with one or more logical 
unit numbers  (LUNs). In iSCS I, LUNs appear as  the target's sequentially numbered disk drives, 
al though targets typically have only one  LUN. An  initiator performs SCSI negotiation with a 
target to  establish a  connection to  the LUN. The LUN responds as  an emulated SCSI  disk block 
device, which can be used in raw form or formatted with a client-supported file system. 

iSCS I  target  configuration 
Target server configuration demonstration 
targetcli  is both a  command-line utility and an interactive  shell  in which to create, delete, and 
configure  iSCSI  target components. Target stack objects are grouped  into a  hierarchical  tree of 
objects, allowing easy navigation and contextual configuration. Familiar Linux commands are 
used  in this shell: cd, ls, pwd, and set. 
The targetcli shell  al so supports TAB completion. Administrators can use TAB to either 
complete partially  typed commands or view a  list of acceptable key-words at the current  location 
in a  command.

The targetcli shell  also supports TAB completion. Administrators can use TAB to either 
complete partially  typed commands or view a  list of acceptable key-words at the current  location 
in a  command.

Install targetcli if needed.
Run targetcli with no options to enter interactive mode. 

Create backing storage (backstores) . There are several  types of  backing storage. 
•  block  - A  block device defined on  the server. A  disk drive, disk partition, a  logical volume, 
multipath device, any device files defined on the server that are of type b. 
•  fileio - Create a  file,  of a  specified size,  in the filesystem of the server. This method  is 
similar to using  image files  to be the storage for virtual machine disk images. 
•  pscsi - Physical  SCS I. Permits passthrough to a  physical  SCS I  device  connected to the 
server.  This backstore type is not  typically used. 
•  ramdisk - Create a  ramdisk device, of a  specified size,  in memory  on the server. This 
type of storage will not store data persistently. When the server is rebooted, the ramdisk 
definition wi ll return when the target  is instantiated, but all data will have been  lost.









Linux  containers 
A  container  is a  lightweight application isolation mechanism that  allows the kernel to run groups 
of processes  in their own  isolated user spaces, separate from the host system. The container 
has its own process  list, network stack, file systems, and other resources, but shares the kernel 
with the host and the other containers  running on the system. Linux containers are  implemented 
through  a  combination of three kernel features: namespaces for isolation, control groups for 
resource control, and SELinux for security. 
A  tool called Docker is used to  create, control, and manage containers. Docker adds an API ,  an 
image format, and a  delivery  and sharing model to Linux containers. A Docker image contains 
an application and all its dependencies. When a  container is started, a  read-write  layer for that 
container  is  combined with the read-only image using LVM  thin provisioning. Docker images are 
portable and can be  saved and exported to other systems and users. 

Namespaces 
The kernel provides  container isolation through names paces, which create a  new environment 
with a  unique view of a  subset of  the resources on the system. Some have called namespaces 
"chroot on steroids,"  since  instead of just  providing processes with a  different root file system, 
processes al so have a  different view of the process table,  networking stack, and so on. 
There are  five different types of namespaces currently in use by containers: 

Control groups 
Control  groups  (cgroups) are  used by  the kernel to manage system  resources. Cgroups allow  fair 
(or unfair)  allocation of CPU time, memory, and  I/O Bandwidth among processes and groups of 
processes. Containers  use cgroups to manage resource consumption, so that a  container will get 
a  certain share of system resources but not  steal all system resources. 
Management of cgroups is beyond the scope of this course. System  administrators familiar with 
cgroups  should be aware that RHEL 7  now uses systemd scope and slice units to more  easily 
manage cgroups. 


SELinux and containers 
Not  everything  is  namespaced, and containers  are  less secure  than virtual machines.  In order to 
protect the host  and other containers  from a  compromised container,  SELinux is used. 
With SELinux enforcing, container processes can  only write to container files. Container 
processes run as the type svirt_lxc_net_t, and image files  are  labeled with the type 
svirt_sandbox_file_t. 
To  keep two containers from accessing each other,  even  though they  have  the same SELinux 
type, Multi-Category  Security (MCS) types are used. Each container runs as svirt_lxc_net_t, 
but is assigned a  random and unique category when it is started by Docker.  If a  process has the
right SELinux type but  its category does not match, access  is denied. A  reference at the end of 
this section provides a  high-level overview of SELinux MCS type checking.

Docker 
Docker provides the user interface, API, image format, and other tools used to manage Linux 
containers  in RHEL 7. The docker package is provided by a  special repository, RHEL 7  Extras 
(rhel-7-server-extras-rpms), which is updated under more aggressive engineering criteria 
than  the core  distribution. 
Docker runs a  daemon started by the systemd  unit docker.service to manage containers. 
The user  interacts with  this daemon through the docker command. Images are  stored  in  a  local 
index kept in the /var /lib/docker directory,  but are  loaded and exported with the docker 
command. 

Docker  images 
A  Docker image  is a  static snapshot of a  container's configuration, which is used to  launch a 
container. The image  is a  read-only layer that  is  never modified.  Instead, Docker adds a  read­
write overlay to which all changes  are made. Changes  are saved by creating a  new  image. A 
single image can be used to generate many  containers that are very slightly different, but only 
need enough disk space to store a  very  small amount of differences.

A platform  image  is an  image with  no parent. This  is a  baseline  image  that defines the core 
runtime environment, packages, and utilities needed to  run an application. RedHat provides 
an extremely minimal platform  image for RHEL 7  that has the minimum required  packages to 
support a  container,  bash, and  to use yum to download and  install additional packages  into a 
container. 

The Docker registry 
Platform  images  can be  provided as a  tar archive  and loaded manually. More frequently,  they 
are pulled from a Docker registry.  This repository may be available to the public. Red Hat provides
a  read-only  registry at registry.access.redhat.com,  from which the RHEL 7  base platform 
image can be pulled. Docker maintains a  public registry from which open source community­
provided  images can be pulled. 
It is also possible to use the docker-registry package to set up a  simple private  repository.  Images 
can  be pushed to this private repository and pulled  by other machines  in the  network. 

Containers  and  virtualization 
Containers  and virtualization are two technologies that provide complementary ways of  dividing 
up system  resources. Containers  can be run on virtual machines and  in cloud computing 
environments, combining both technologies.  If virtualization "vertically" abstracts hardware, 
containers  can  be said to "horizontally" segment operating systems. 
There are use cases more  suited for one technology or the other.  Some advantages of Docker 
containers: 
Containers are  lightweight  in resource use, so more  containers  can be run on the same 
hardware. 
•  Containers can  be created and  destroyed more  quickly than virtual machines. 
Un like  virtual machines, containers  do not need to support an entire operating system; only a 
core runtime  is  needed for the application. This allows rapid application deployment. 
Docker images have a  version control stream, so successive versions of an image can be 
tracked and even reverted. Components reuse components from other layers, making 
container images  very  lightweight. 
Docker images are easily transferable to other machines which have Docker available.

By comparison, some advantages of virtualization: 
Virtual machines  run their own kernel and  full operating system, which allows stronger 
isolation between the host hypervisor and the virtual machine. 
Virtual machines can easily run operating systems and kernels that  are completely different 
than the hypervisor host's operating system. 
Virtual machines can be live-migrated  from one hypervisor node to another while running; 
containers must be stopped before  being moved  from one machine to another.

Using Docker

[root@demo  -)#  subscription-manager repos  --enable=rhel-7-server-extras-rpms 
[root@demo  -]#  subscription-manager repos  --enable=rhel-7-server-optional-rpms 
[root@demo  -)#  yum  install  docker

[root@demo  -)#  systemctl  start  docker ;  systemctl  enable  docker

Pu ll ing or loading platform  images 
Once  Docker is installed, the next step is to  get  base platform  images  into the machine's local 
index. One way to do  this  is to  pull the images from a  remote Docker registry. This can be  done 
with the command docker  pull  repository/image -name.  If no repository is specified, 
the community Docker hub repository is assumed. Red Hat has an official base image named 
redhat/rhel7 available at registry.access.redhat.com. 

[root@demo  -]#  docker  pull  registry.access.redhat.com/redhat/rhel7

docker  load  -i  rhel-server-docker-7.0-21.4-x86_64.tar.gz

[root@demo  -]#  docker  images 
REPOSITORY									TAG			IMAGE ID		CREATED		VIRTUAL SIZE 
registry.access.redhat.com/redhat/rhel7		0			e1f5733f050b  	3 days ago  140.2 MB 
registry.access.redhat.com/redhat/rhel7		0-21		e1f5733f050b  	3 days ago  140.2 MB

Running  Docker containers 
The docker  run command  is  used to  run a  container.  In its simplest form,  it  takes two 
arguments: the name of the  image to use, and the  name of a  command to run  in the container. 
Here  is  an example that will run a  container in interactive mode,  using the image rhel7, and run 
the command cat  /etc/hosts. When the command exits, the container will be stopped. The 
- - rm command will delete the container when it exits.

[root@demo  -]#  docker  run  -i  -t  --rm  rhel7  cat  /etc/hosts

A more  useful  example will start a  new container,  running the bash command:

[root@demo  -]#  docker  run  -i  -t  rhel7  bash 
bash -4 .2#  echo  hello  >  /tmp/testfile 
bash -4 .2#  exit 
exit 
[root@demo  -]#

This time, when the container exited, it did not  remove its file system because the  --rm option 
was not  included. This can  be seen with the docker  ps  -a command. The  -a  option shows 
all containers  on the system, running or stopped. (Without  -a, only running containers  are 
displayed.)

Creating Docker images 
In the next example, a  Docker image will be  created from the base image that includes the httpd 
package. The new image will then be  used to  run a  container. 
Run a  container from the base RHEL 7  image once, to manually install the httpd package. 

docker run -i  rhel7 bash -c "yum  install  -y  httpd ;  yum  clean  all"

After  the packages  have installed and the container has exited,  identify the last container 
which wa s  run with docker  ps  -l. With the name or ID of the container,  the docker  commit 
command can  be used to create the  new image from the container's file system. The following 
command will also include a  comment (with  -m). 

[ root@demo  -]#  docker  ps  -1 
CONTAINER ID  IMAGE				COMMAND						CREATED			STATUS  PORTS  				NAMES 
ecaf8a3102a3  redhat/rhel7:0	bash  - c  'yum  clean  a 4  minutes  ago	Exited (0) 3 minutes ago  dreamy_goodall1

[root@demo  -]#  docker  commit  -m  "RHEL7  +  httpd"  dreamy_goodall1  rhel_httpd 
74acf538ab6f5308a77bc308abae753e4ee683583eb4821455bccd4a141a7591 

[root@demo  -]#  docker  images 
REPOSITORY  TAG  IMAGE  ID  CREATED  VI RTUAL  SIZE 
rhel_httpd  latest  74acf538ab6f  52  seconds  ago  168.2  MB 
registry.access.redhat.com/redhat/rhel7  0  e1f5733f050b  3  days  ago  140 .2  MB 
registry.access.redhat.com/redhat/rhel7  0 - 21  e1f5733f050b  3  days  ago  140.2  MB


Now a  container can be run which uses this new  image as its baseline. The following command 
will  introduce two new options. The  -p 8080:80 option will map port 8080/TCP on the host to 
port 80/TCP on the container. The  -d will cause the container to start in the background. The 
command /usr/sbin/httpd  -DFOREGROUND will start Apache HTTPD in a very basic mode in 
the container. 


Starting a  private Docker registry 
To make it  easier to  distribute finished images to  other hosts, system administrators can push 
their images to a  Docker registry.  RedHat provides a  package, docker-registry, which can be used 
to set up a  private registry for an organization's internal use. This server does not  need to be on 
a  host running Docker itself. 

Pushing  images  to a  private  registry 
To  push an  image to a  private repository,  it must first be tagged with the host address and  port 
of  the repository with docker  tag. It can then be  pushed with docker  push.

Exporting  images  to a  file 
The docker  save command can be used to save an image file to an uncompressed tar archive. 
This file can  then be copied to another Docker host and loaded into its index with docker  load. 
This archive will include all the  layers needed to reconstruct the  image.

Cleaning up 
Several commands are  useful for cleaning up  containers: 
•  docker  stop  container will stop a  container gracefully,  and docker  kill  container 
will send it a  kill signal. 
•  docker  rm  container will delete a  container's file system  image permanently from the 
local index. Once this is done, the container can not  be restarted; it must be recreated. 
•  docker  rmi image will remove an image from the host's index. It does not  remove that 
image from a  registry.  It is important not to remove any images that a host may currently be 
using. 
•  docker  info will provide some basic information about the Docker environment and current 
resource consumption. 
