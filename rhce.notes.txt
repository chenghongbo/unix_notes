Features provided by systemd:

•  Parallelization capabilities, which increase the boot speed of a  system. 
•  On-demand starting of daemons without requiring a  separate service. 
•  Automatic service dependency management prevents  long  timeouts, such as not starting a network service when the network  is  not available. 
•  A method of tracking related processes together using Linux control  groups. 



 
[root@serverx  -]#  systemctl  status  sshd.service 
'  sshd.service  - OpenSSH  server  daemon 
	Loaded :  loaded  (/usr/lib/systemd/system/sshd.service;  enabled ) 
	Active :  active  (running )  since  Thu  2014-02-27  11:51:39  EST;  7h  ago 
	Main  PID : 1073 (sshd ) 
	CG group : /system.slice/sshd.service 
			   L. 1073  /usr/sbin/sshd  -D 
Feb  27  11 :51:39  server0.example.com  systemd [1] :  Started  OpenSSH  se rver  daemon . 
Feb  27  11 :51:39  server0.example.com  sshd [1073] :  Could  not  load  host  key :  /et  ...  y 
Feb  27  11 :51:39  server0.example.com  sshd [1073] :  Server  lis tening  on  0.0.0.0  ...  . 
Feb  27  11 :51:39  server0.example.com  sshd [1073] :  Server  listening  on  ::  port  22 . 
Feb  27  11 :53:21  server0.example.com  sshd [1270] :  error:  Could  not  load  host  k ...  y 
Feb  27  11 :53:22  server0.example.com  sshd [1270] :  Accepted  pas swo rd  for root  f ...  2 
Hint :  Some  lines  were  ellipsized ,  use  -1  to  show in  full . 

-------------------------------------------------------------------------------------------------
Keyword:  			|	Description: 
-------------------------------------------------------------------------------------------------
loaded  			|	Unit  conf iguration file has been processed. 
active (running)  	|	Running with one or more  continuing processes. 
active (exited)  	|	Successfully completed a  one-time configuration. 
active (waiting)  	|	Running but waiting for an event. 
inactive  			|	Not  running. 
enabled  			|	Will be started at boot time. 
disabled  			|	Will not be started at boot time. 
static  			|	Can not be enabled, but may be started by an enabled unit automatically. 
-------------------------------------------------------------------------------------------------


Query  the state of  all units to verify a  system startup. 
  [root@serverx  -]#  systemctl 

Query the state of only the service units. 
[ root@serverx  - ]#  systemctl -type=service 

Investigate any units which are  in a  failed or maintenance  state. Optionally,  add the  -1  option to show the full output. 
 [root@serverX  -]#  systemctl  status  rngd.service  -1 

The status argument may also be  used to  determine if a  particular unit is  active and show  if 
the unit  is enabled to start at boot time. Alternate commands can also easily show the active and enabled  states: 

[root@serverx  -]#  systemctl  is-enabled  sshd
[root@serverx  -]#  systemctl  is-active  sshd

View the enabled and disabled  settings for all units. Optionally,  limit  the type of  unit. 

systemctl  list-unit-files  --type=service

See only failed services：
systemctl  --failed  --type=service



[root@serverx  -]#  systemctl start sshd.service
[root@serverx  -]#  systemctl stop sshd.service

[root@serverx  -]#  systemctl restart sshd.service
[root@serverx  -]#  systemctl staus sshd.service

[root@serverx  -]#  systemctl reload sshd.service


Unit dependencies 
Services may be  started  as  dependencies of other services.  If a socket unit is enabled  and the 
service unit with the same name is not, the service will automatically  be started when a  request 
is made on the network socket. Services may also  be triggered by path units when a  file system 
condition is met. 

The systemctl  list-dependencies  UNIT 

command can be used to display a  tree of other 
units which must  be started in conjunction with a  specific unit. The  --reverse option to this 
command will show what units need to have the specified unit started in order to run.

 
Masking services 
A system may have conflicting services installed for a certain function, such as firewalls (iptables 
and firewalld). To  prevent an administrator from accidentally starting a  service, a  service may be 
masked. Masking creates a  link in the configuration directories so that if the service is started, 
nothing will happen.

[root@serverx  -]#  systemctl mask network
ln -s '/dev/null' '/etc/systemd/system/network.service'


---------------------------------------------------------------------------------------------------------------------
Command:  					        |  Task: 
---------------------------------------------------------------------------------------------------------------------
systemctl  status  UNIT  			|	View detailed information about a  unit state. 
systemctl  stop  UNIT  	 			|	Stop a  service on a  running  system. 
systemctl  start  UNIT   			|	Start a  service on a  running system. 
systemctl  restart  UNIT  			|	Restart a  service on a  running system. 
systemctl  reload  UNIT  			|	Reload configuration file of a  running service. 
systemctl  mask  UNIT    			|	Completely disable a  service from being started, both manually and at boot. 
systemctl  unmask  UNIT  			|	Make a masked service available. 
systemctl  enable  UNIT  			|	Configure a  service to  start at  boot time. 
systemctl  disable  UNIT 			|	Disable a  service from starting at  boot time. 
systemctl  list-dependencies UNIT	| List units which are  required and wanted by the specified unit. 
---------------------------------------------------------------------------------------------------------------------


A systemd target  is a set of systemd units that should be started to reach a  desired state. 
Important targets are  listed  in the  following table. 

-----------------------------------------------------------------------------------
Target  			|	Purpose 
-----------------------------------------------------------------------------------
graphical.target  	|	System supports multiple users, graphical and text-based logins. 
multi-user.target  	|	System supports multiple users, text-based  logins only. 
rescue.target  		|	sulogin prompt. basic system  initialization completed. 
emergency.target  	|	sulogin prompt. initramfs pivot complete and system root mounted on / read-only. 
-----------------------------------------------------------------------------------

[root@serverX  -]#  systemctl  list-units  --type=target  --all

On a running system, administrators  can choose to switch to a  different target using the 
systemctl  isolate command; for example,  

	systemctl  isolate multi-user.target 
	
Selecting a  different target at  boot time
 
To select a  different target at boot time, a  special option can be appended to the kernel 
command line from the boot loader: systemd.unit=. For example,  to boot  the system  into a 
rescue shell, pass the following option at  the interactive boot  loader menu: 

systemd.unit=rescue.target 


To  use this method of selecting a  different target, use the following procedure for Red Hat 
Enterprise Linux 7  systems: 
1.  (Re)boot the system. 
2.  Interrupt the boot  loader menu  countdown by pressing any key. 
3.  Move the cursor to  the entry to be started. 
4.  Press e to edit  the current entry. 
5.  Move  the cursor to the  line that starts with linux16. This  is the kernel command li ne. 
6.  Append systemd.unit=desired.target. 
7.  Press Ctrl +x  to  boot with these changes.


--- reset lost root password:

On Red Hat Enterprise Linux 7,  it is  possible to  have the scripts that run from the initramfs 
pause at certain points, provide a  root shell, and then continue when that shell exits. While this 
is mostly meant  for debugging,  it can also be used to recover a  lost root password: 
1.  Reboot the system. 
2.  Interrupt  the boot  loader countdown by pressing any  key. 
3.  Move the cursor to the entry  that needs to be booted. 
4.  Press e to edit the selected entry. 
5.  Move the cursor to the kernel command line (the  line that  starts with linux16. 
6.  Append rd.break (this will break just  before control  is handed  from the initramfs to the actual system). 
7.  Press Ctrl +x  to  boot with the changes.
 
At this point, a  root  shell will be presented, with the  root file system for the actual system mounted read-only on /sysroot.

To recover  the root password from this point. use the following procedure: 
1.  Remount /sysroot as read-write 
		mount  -oremount,rw  /sysroot
2. Switch  into a  chroot jail, where /sysroot  is treated  as the  root of the file system tree. 
		chroot /sysroot
3. Set a new root password: 
		passwd root
4. Make sure that all unlabeled files  (including /etc/shadow at this point) get relabeled during boot.
		touch  /.autorelabel 
5. Type exit twice. The first will exit the chroot jail, and the second will exit the initramfs debug shell. 

Diag nose and repair systemd boot  issues 

If there are problems during the starting of services, there are a few tools available to system 
administrators that can help with debugging and/or troubleshooting: 

Early debug shell 
By running systemctl  enable  debug-shell.service, a  root shell will be spawned on 
TTV9  (Ctrl+Alt+F9) early during the boot  sequence. This shell is automatically logged in as 
root so that an administrator can use some of the other debugging tools while the system  is still booting. 

Emergency and  rescue ta rgets 
By appending either systemd.unit=rescue.target or systemd.unit=emergency.target to  the  kernel command 
line from the boot loader, the system will spawn into a special rescue or emergency shell instead of starting normally.  Both of these shells require the root password. The emergency target keeps the root file system 
mounted read-only, while rescue.target waits for sysinit.target to complete first so that more of the system will be 
initialized, for example,  logging, file systems, etc. Exiting from these shells will continue with the  regular boot process.

Stuck  jobs 
During startup, systemd spawns a number of jobs.  If some of these jobs can not complete, they 
will block other jobs from running. To inspect the current job list. an administrator can use the 
command systemctl list-jobs. Any  jobs listed as running must complete before the jobs 
listed as waiting can continue. 







In  Red Hat Enterprise Linux 7, the configuration of network  interfaces is managed by a  system 
daemon called NetworkManager.  For  NetworkManager: 
•  A device  is a  network interface. 
•  A connection  is a  collection of settings that can be configured for a  device. 
•  Only one connection is active  for any one device at a  time. Multiple connections may exist, for 
use by different devices or to allow a  configuration to be altered for the same device. 
•  Each connection has a name or  ID  that identifies it. 
•  The persistent conf iguration for a  connection is stored  in 
/etc/sysconfig/network- scripts/ifcfg - name, where name  is the name of the 
con nection (although spaces are  normally  replaced with underscores in the file name). This file 
can  be edited by hand if desired. 
•  The nmcli utility can be used to create and edit connection files  from the shell prompt. 



------------------------------------------------------------------------------------------------------------------
Command  			Purpose
------------------------------------------------------------------------------------------------------------------
nmcli dev status		Show the NetworkManager status of all network interfaces. 
nmcli con show  		List  all connections. 
nmcli con show name  		List  the current settings for  the connection name. 
nmcli con add con-name name 	Add a  new connection named name. 
nmcli con  mod name ...  	Modify the connection name. 
nmcli con  reload  		Tell NetworkManager to reread the configuration files
nmcli con  up  name		Activate the connection name. 
nmcli dev  dis  dev  		Deactivate and disconnect the cu rrent connection on the network interface dev. 
nmcli con  del  name		Delete the connection name and its configuration file. 
ip  addr  show			Show  the current network  interface address configuration. 
hostnamectl set-hostname	Persistently set the host name on this system. 
------------------------------------------------------------------------------------------------------------------


Network teaming
Network teaming is method for linking NICs together logically to allow for failover or  higher 
throughput. Teaming is a  new implementation that does not affect the older bonding driver in 
the Linux kernel; it offers an alternate implementation. Red Hat Enterprise  Linux 7  supports 
channel bonding for  backward compatability. Network teaming provides better  performance and 
is more extensible because of its modular design. 
Red Hat Enterprise Linux 7  implements network teaming with a  small kernel driver and a  user­
space daemon, teamd. The kernel handles network packets efficiently and teamd handles logic 
and interface processing. Software, called runners,  implement load balancing and active-backup 
logic, such as round robin. The following runners are available to teamd:

· broadcast: a simple runner which transmits each packet from all ports. 
. round robin: a simple runner which transmits packets in a round-robin fashing from each of the ports. 
· activebackup: this is a failover runner which watches for link changes and  selects an active port for data transfers. 
· loadbalance: this runner monitors  traffic and uses a hash function to try to reach a  perfect 
balance when selecting ports for packet transmission. 
· lacp: implements the 802.3ad Link Aggregation Control Protocol. Can use the same transmit port selection possibilities as the loadbalance runner.



All network interaction is done through a team interface, composed of multiple network port 
interfaces. When controlling teamed port interfaces using NetworkManager,  and especially when 
fault finding, keep the following in mind: 
•  Starting the network team interface does not automatically start the port interfaces. 
•  Starting a port interface always starts the teamed interface. 
•  Stopping the teamed interface also stops the port interfaces. 
•  A teamed interface without ports can start static IP connections. 
•  A team without ports waits for ports when starting DHCP connections. 
•  A team with a DHCP connection waiting for ports completes when a port with a carrier is added. 
•  A team with a DHCP connection waiting for ports continues waiting when a port without a carrier is added. 

Configuring network teams 
The nmcli command can be used to create and manage team and port interfaces. The following 
four steps are used to create and activate a network team interface: 
1.  Create the team  interface. 
2.  Determine the  1Pv4 and/or 1Pv6 attributes of  the team  interface. 
3.  Assign the port interfaces. 
4.  Bring the team  and port interfaces up/down. 

Create the team interface 

Use  the nmcli command to create a connection for the network team  interface, with the 
following syntax: 

		nmcli con add type team con-name CNAME ifname INAME [config  JSON] 
		
where CNAME will be  the name used to refer to  the connection, INAME will be  the interface name, 
and JSON specifies the runner to be  used. JSON has the following syntax:

 
	'{ "runner":  {"name": "METHOD"}} ' 
	
where METHOD  is one of  the following: 
	broadcast, roundrobin, activebackup, loadbalance, or lacp.
	
[root@demo -)#  nmcli con add type team con-name team0 ifname team0 config  '{"runner": {"name": "loadbalance"}}'

Determine the IPv4/IPv6 attributes of  the team interface 
Once  the network team  interface is created, IPv4 and/or IPv6 attributes can be assigned to it. If 
DHCP is available, this step is optional, because the default attributes configure the interface to 
get its IP settings using DHCP

The following example demonstrates how to assign a static  IPv4 address  to  the team0  interface:

[root@demo -]# nmcli con  mod  team0  ipv4.addresses  1.2.3.4/24  
[root@demo -]# nmcli con  mod  team0  ipv4.method  manual


Assign ports to team interface:
	nmcli con add type team-slave  con-name CNAME ifname INAME master TEAM
	
When  the team  interface is up, the teamdctl command can be used to display the team's state. 
The output of this command includes the status of the port interfaces.


NetworkManager creates configuration files  for  network teaming in the 
/etc/sysconfig/network-scripts the same way it does for other interfaces. A 
configuration file is created for each of the interfaces: for the team, and each of the ports. 
The configuration file for the team  interface defines the IP settings for the interface. The 
DEVICETYPE variable informs the  initialization scripts this is a network team interface. 
Parameters for teamd configuration are defined  in the TEAM_CONFIG variable. Note that the 
contents of TEAM_CONFIG uses JSON syntax.

		# /etc/sysconfig/network-scripts/ifcfg-team0 
		DEVICE=team0 
		DEVICETYPE=Team 
		TEAM_CONFIG="{\"runner\":  {\"name\": \"broadcast\"}}" 
		BOOTPROTO=none 
		IPADDR0=172 .25.5.100 
		PREFIX0=24 
		NAME=team0
		ONBOOT=yes  
		
		# /etc/sysconfig/network-scripts/ifcfg-team0-eth1 
		DEVICE=eth1 
		DEVICETYPE=TeamPort 
		TEAM_MASTER=team0 
		NAME=team0-eth1 
		ONBOOT=yes
		

Initial  network team  configuration is set when the team  interface is created. The default  runner 
is roundrobin, but a different runner can be chosen by specifying a JSON string when the team 
is created with the team.config subcommand. Default values for runner parameters are  used 
when they are not specified.

		nmcli con mod IFACE team.config  JSON-configuration-file-or-string
		
# cat /tmp/team.conf	
{ 
	"device": "team0", 
	"mcast_rejoin": { 
		"count": 1 
	}, 
	"notify_peers" : { 
		"count": 1 
	}, 
	"ports": { 
		"eth1": { 
	"prio": -10, 
	"sticky": true, 
		"link_watch": { 
			"name": "ethtool" 
			} 
		},
		"eth2": { 
	"prio": 100, 
		"link_watch": { 
			"name": "ethtool"
				}
			}
		},
		"runner": { 
			"name": "activebackup"
			}
	}
[root@demo -]# nmcli con mod team0 team.config  /tmp/team.conf 



The link_watch settings in the configuration file determines how the  link state of the port 
interfaces are monitored. The default  looks like  the following, and uses functionality similar to 
the ethtool command to check the  link of each interface: 

	"link_watch": { 
		"name": "ethtool"
		}

Another way to  check link state  is to  periodically use an ARP ping packet to  check for remote 
connectivity.  Local  and remote  IP addresses and timeouts would have to be specified. A 
configuration that would accomplish that would look similar to  the following:

	"link_watch":{ 
		"name": "arp_ping", 
		"interval":  100, 
		"missed_max": 30, 
		"source_host": "192.168.23.2", 
		"target_host": "192.168.23.1"
	}, 
	
	
	
Display  the team  ports of the team0  interface:	
[root@demo -]# teamnl  team0  ports
4: eth2:  up 0Mbit HD 
3: eth1:  up 0Mbit HD 

Display the currently active port of team0:
[root@demo -]# teamnl  team0  getoption  activeport 
3 

Set the option for the active port of team0:
[root@demo -]# teamnl team0 setoption activeport 3

Use teamdctl to display the cu rrent state of the teama  interface: 

		[root@demo -]# teamdctl team0  state 
		setup: 
			runner:  activebackup 
		ports : 
			eth2 
				link watches: 
					link summary: up 
					instance[link_watch_0] : 
					name: ethtool 
					link: up 
			eth1
				link watches: 
					link summary: up 
					instance[link_watch_0] : 
						name: ethtool 
						link: up 
		runner: 
			active  port: eth1
			
			
teamdctl team0 config dump



Software bridges 

A network bridge is a link-layer device that forwards traffic between networks based on MAC 
addresses.  It learns what hosts are connected to  each network, builds a table of MAC addresses, 
then makes packet forwarding decisions based on that table. A  software bridge can be used in 
a  Linux environment to emulate a  hardware  bridge. The most common application for software 
bridges is in virtualization applications for sharing a hardware NIC among one or more virtual 
NICs. 

Configure  software  bridges 
The nmcli can be used to configure software bridges persistently.  First. the software bridge is 
created, then existing interfaces are  connected to it. For example,  the following commands will 
create a  bridge called br0  and attach both the ethl and eth2 interfaces to  it. 


[root@demo  -]#  nmcli  con add type bridge con-name  br0  ifname  br0 
[root@demo  -]#  nmcli  con add type bridge-slave  con-name  br0-port1  ifname  eth1  master  br0
[root@demo  -]#  nmcli  con add type bridge-slave  con-name  br0-port2  ifname  eth2  master  br0


Softwa re bridge configuration files 
Software bridges are managed by interface configuration files found in the 
/etc/sysconfig/network-scripts directory. There  is an ifcfg-*  configuration file for 
each software bridge. 
The following  is a sample configuration file for a  software bridge: 

		# /etc/sysconfig/network-scripts/ifcfg-br1
		DEVICE=br1 
		NAME=br1 
		TYPE=Bridge 
		BOOTPROTO=none 
		IPADDR0=172.25.5.100 
		PREFIX0=24 
		STP=yes 
		BRIDGING�OPTS=priority=32768
		
The following configuration file attaches an Ethernet  interface to a software bridge: 	
		# /etc/sysconfig/network-scripts/ifcfg-br1-port0
		TYPE=Ethernet 
		NAME=br1-port0 
		DEVICE=eth1 
		ONBOOT=yes 
		BRIDGE=br1
		
=====================  NOTE =====================	
To implement a  software bridge on  an existing teamed or  bonded network interface 
managed by NetworkManager,  NetworkManager wi ll have to be disabled since it only 
supports bridges on simple Ethernet  interfaces. Configuration files for  the bridge will 
have  to be created by hand. The ifup and ifdown utilities can be used to manage the 
software bridge and other network interfaces. 


The brctl show command will display  software bridges and the  list of  interfaces attached to 
them. 

[root@demo  -]#  brctl  show 
bridge name		bridge id 				STP enabled		interfaces
br1  			8000.52540001050b		yes				eth1

# Ping  the local network gateway using the software bridge. 
[root@serverX  -]#  ping  -I  brl  192.168.0.254 
PING  192 .168.0.254  (192.168.0.254 )  from  192 .168.0.100  brl :  56 (84)  bytes  of  data . 
64  bytes  from  192 .168.0.254 :  icmp_seq=10  ttl=64  time=0 .520  ms 
64  bytes  from  192 .168.0.254 :  icmp_seq=11  ttl=64  time=0 .470  ms 
64  bytes  from  192 .168.0.254 :  icmp_seq=12  ttl=64  time=0 .339  ms 
64  bytes  from  192 .168.0.254 :  icmp_seq=13  ttl=64  time=0 .294  ms 
...  Output  omitted  ... 



firewalld  is the default method in Red Hat Enterprise Linux 7  for managing host-level 
fi rewa lls. Sta rted from the firewalld.service systemd service, firewalld manages the 
Linux kernel netfilter subsystem  using the low-level iptables, ip6tables, and ebtables 
commands.

=================== NOTE ===============
The firewalld.service and iptables.service, ip6tables.service, and 
ebtables.service services conflict with each other. To  prevent accidentally starting 
one of the *tables.service services (and wiping out any running firewall config  in 
the process), it is good practice  to mask them using  systemctl. 

[root@serverX  -]#  for  SERVICE  in  iptables  ip6tables  ebtables ;  do 
>  systemctl  mask  ${SERVICE}.service 
>  done 


firewalld separates all incoming traffic  into zones, with each zone having  its own set of rules. 
To check which zone to use for an incoming connection, firewalld  uses this  logic, where the 
first  rule that matches wins: 

1.  If the source address of an  incoming packet matches a  source  rule setup for a  zone, that 
packet will  be routed through that zone. 
2.  If the incoming interface for a  packet matches a  filter setup for a zone, that zone will be 
used. 
3.  Otherwise, the default zone  is used. The default zone  is not a  separate zone;  instead,  it 
points to one of the other zones defined  on the system.
 
Unless overridden by an administrator or a  NetworkManager configuration, the default zone for 
any new network interface will be set to the public zone. 


Zone  name  		Default configuration 
---------------------------------------------------------------------------------------------------
trusted				Allow all incoming traffic. 
---------------------------------------------------------------------------------------------------	
home 				Reject incoming traffic unless  related to outgoing traffic or matching 
					the ssh, mdns, ipp-client, samba-client, or dhcpv6-client predefined services. 
---------------------------------------------------------------------------------------------------
internal  			Reject incoming traffic unless related to outgoing traffic or matching 
					the ssh, mdns, ipp-client, samba-client, or dhcpv6-client predefined services。
---------------------------------------------------------------------------------------------------					
work  				Reject  incoming traffic unless related to outgoing traffic or matching 
					the ssh, ipp-client, or dhcpv6-client predefined services.					
---------------------------------------------------------------------------------------------------
public  			Reject  incoming traffic unless  related to outgoing traffic or matching 
					the ssh or dhcpv6-client predefined services. The default zone for newly added 
					network interfaces.
---------------------------------------------------------------------------------------------------					
external  			Reject incoming traffic unless related to outgoing traffic or matching 
					the ssh predefined service. Outgoing  IPv4 traffic forwarded through 
					this zone  is masqueraded to  look like  it originated from the  IPv4 
					address of the outgoing network  interface. 
---------------------------------------------------------------------------------------------------					
dmz  				Reject incoming traffic unless related  to outgoing traffic or matching 
					the ssh predefined service. 
---------------------------------------------------------------------------------------------------					
block  				Reject all incoming traffic unless  related  to  outgoing traffic.
---------------------------------------------------------------------------------------------------					
drop  				Drop all  incoming traffic unless related to outgoing traffic (do not even 
					respond with  ICMP errors). 					
---------------------------------------------------------------------------------------------------	


Managing firewalld 
firewalld  can be managed  in three ways: 
1.  Using the command-line tool firewall-cmd. 
2.  Using the graphical tool firewall-config. 
3.  Using the configuration files  in /etc/firewalld/

In most cases, editing the configuration files directly is not  recommended, but  it can be useful to 
copy configurations in this way when using configuration management tools. 

Configure  firewall settings with firewall-cmd 

This section will focus on managing firewalld  using the command-line tool firewall- cmd. 
firewall-cmd  is installed as part of the main firewalld package. firewall-cmd  can  perform 
the same actions as firewall-config. 
The fol lowing table lists a  number of frequently used firewall-cmd commands, along with an 
explanation. Note that unless otherwise specified, almost all commands will work on the runtime 
configuration, unless the --permanent option is specified. Many of the commands listed take 
the --zone=<ZONE>  option to determine which zone  they affect.  If --zone is omitted from 
those commands, the default zone is used. 
While configuring a  firewall, an administrator will normally apply all changes to the  - -
permanent configuration, and then activate  those changes with firewall-cmd  --reload. 
While testing out new, and possibly dangerous, rules,  an administrator can choose to work 
on the runtime configuration by omitting the  --permanent  option. In those cases, an extra 
option can be  used to automatically remove a  rule after a  certain amount of time, preventing an 
administrator from accidentally locking out a  system:  - -timeout=<TIMEINSECONDS>。

fi rewa ll -cmd example 
The followi ng examples  show  the default  zone being set  to dmz, all traffic coming from the 
192.168.0.0/24 network being assigned to the in ternal zone, and the network ports for 
mysql being opened on the internal zone. 


[root@serverX  -]#  firewall-cmd --set-default --zone=dmz 
[root@serverx  -]#  firewall-cmd --permanent  --zone=internal  --add-source=192.168.0.0/24
[root@serverX  -]#  firewall-cmd --permanent  --zone=internal  --add-service=mysql 
[root@serverx  -]#  firewall-cmd --reload 

Rich rules  concepts 
Apart  from the regular zones and services syntax that firewalld offers, administrators  have 
two other options  for adding  firewall rules: direct rules  and rich rules. 

Direct  rules 
Direct rules allow an administrator to insert hand-coded {ip,ip6,eb}tables rules  into 
the zones managed by firewalld. While powerful, and exposing features of the kernel 
netfilter subsystem not exposed through other means, these  rules  can be hard  to manage. 
Direct rules  also offer less flexibility than standard  rules and rich rules. Configuring direct 
rules is not covered  in this course, but documentation is available in the firewall-cmd(1) 
and firewalld.direct (5) man pages for those administrators who are  already familiar with 
{ip,ip6,eb}tables syntax. 
Unless explicitly inserted  into a zone managed by firewalld, direct rules  will  be parsed before 
any firewalld rules are. 

[root@se rverx  -)# firewall-cmd  --direct  --permanent  --add-chain ipv4  raw  blacklist 
[root@serverX  -]#  firewall-cmd  --direct  --permanent  --add-rule  ipv4  raw  PREROUTING 0 -s \
192.168.0.0/24  -j  blacklist 
[root@serverx  -]#  firewall-cmd  --direct  --permanent  --add-rule  ipv4  raw  blacklist  0  -m \ 
limit  --limit  1/min  -j  LOG  -- log-prefix  "blacklisted " 
[root@serverX  -]#  firewall-cmd  --direct  --permanent  --add-rule  ipv4  raw  blacklist  1  -j DROP 

Rich rules 
firewalld rich  rules give administrators an expressive  language in which to express custom 
firewall rules that are  not covered by the basic firewalld syntax; for example,  to only allow 
connections to a  service from a  single IP address, instead of  all IP addresses routed through a zone. 
Rich rules  can be used to ex press basic all ow/deny  rules, but can also  be used to configure 
logging, both to  syslog and auditd, as  we ll as  port forwards, masquerading, and rate limiting. 

The basic syntax  of a  rich rule can  be expressed by the following block: 

	rule 
		[source] 
		[destination] 
		service | port | protocol | icmp-block | masquerade | forward-port 
		[log] 
		[audit] 
		[accept|reject|drop] 

Rule ordering 

Once multiple rules have been added to a zone  (or the firewall in genera l), the ordering of rules 
can have a  big effect on how  the firewall behaves. 
The basic ordering of rules  inside a  zone  is the same  for all zones: 

1.  Any port forwarding and masquerading rules set  for that zone. 
2.  Any  logging rules set for that zone. 
3.  Any allow rules set for that zone. 
4.  Any deny rules set  for that zone. 

In  all cases, the first match will win. If  a  packet has not been matched  by any rule  in  a  zone,  it will 
typically be denied, but zones might have a  different default; for example, the trusted zone will 
accept any  unmatched packet. Also, after matching a  logging rule, a  packet will continue to be 
processed as normal. 
Direct rules are an exception. Most direct rules will be parsed before any other processing is done 
by firewalld,  but the direct rule syntax allows an administrator to  insert any  rule they want 
anywhere in any zone. 

Testing and  debugging 
To make  testing and  debugging easier,  almost all  rules can be added to the runtime configuration 
with a  timeout. The moment the rule with a  timeout  is added to the firewall, the timer starts 
counting down for that  ru le. Once the timer for a  rule  has reached zero seconds, that  rule  is 
removed from the runtime configuration. 
Using timeouts can be an incredibly useful toot while working on a  remote  firewalls, especially 
when testing more complicated  rule  sets.  If a  rule works, the administrator can add  it  again, 
but with the --permanent option  (or at  least without a  timeout).  If the rule does not work 
as intended, maybe even  locking the administrator out of the system, it will be removed 
automatically, al lowing the administrator to continue  his or her work. 
A timeout  is added to a  runtime rule by adding the option  --timeout=<TIMEINSECONDS>  to 
the end of  the firewall-cmd  that enables  the rule

[root@serverX  -]# firewall-cmd  --permanent --zone=classroom  --add-rich-rule='rule family=ipv4  source address=192.168.0.11/32 reject' 

The difference  between reject and drop lies  in the fact that a  reject will send 
back an  ICMP packet detailing that, and why, a  connection was rejected. A drop  just 
drops the packet and does nothing else. Normally an administrator will want to use 
reject for friendly and neutral networks, and drop only for hostile networks


Logging with  rich rules 
When debugging, or monitoring, a  firewall, it  can be  useful to  have a  log of  accepted or  rejected 
connections. firewalld can accomplish this in two ways: by  logging to syslog, or by sending 
messages to the kernel audit subsystem, managed by auditd. 
In both cases, logging can be rate  limited. Rate limiting ensures that system  log files  do not fill up 
with messages at a  rate such that the system can not  keep up, or fills all its disk space. 
The basic syntax  for  logging to syslog using rich rules  is

log  [prefix="<PREFIX  TEXT>"  [level=<LOGLEVEL>] [limit value="<RATE/DURATION>"]

Where <LOGLEVEL>  is  one of emerg, alert, crit, error, warning, notice, info, or debug. 
<DURATION>  can be one of s for seconds, m for minutes, h  for hours, or d  for days. For example, 
limit  value=3/m will limit the log messages to a maximum of three per minute. 
The basic syntax  for logging to the audit subsystem is: 

audit  [limit  value= "<RATE/DURATION>"] 



Network Address Translation  (NAT) 
firewalld  supports two types of Network Address Translation  (NAT): masquerading and port 
forwarding.  Both can be configured on  a  basic level with regular firewall-cmd  rules, and more 
advanced forwarding configurations can be accomplished with rich rules. Both forms of  NAT 
modify  certain aspects of a  packet,  like the  source or  destination, before sending  it  on. 

Masquerading  ( IPV4 only)
With masquerading, a  system will forward packets that are  not directly addressed  to  itself to 
the  intended  recipient, while  changing the source address of the packets that  go through to  its 
own public  IP address. When answers to  those packets  come  in, the firewall will then modify  the 
destination address to the address of the original host, and send the  packet on. This is usually 
used on the edge of a  network to provide  Internet access to an internal network. Masquerading  is 
a  form of Network Address Translation  (NAT). 


To  configure masquerading for a zone with regular firewall-cmd commands, use the following syntax: 

[root@serverX  -]#  firewall-cmd  --permanent  --zone=<ZONE>  -- add-masquerade

This will masquerade any  packets sent to  the firewa ll from clients defi ned in the sou rces for that 
zone (both  inte rfaces and sub nets) that are  not  addressed to the firewall itself. 
To  gain more  control over what clients will be masqueraded, a  rich rule can be  used as  well. 

[root@serverX  -]#  firewall-cmd  --permanent  --zone=<ZONE>  --add-rich-rule='rule 
family=ipv4  source address=192.168.0.0/24  masquerade'



Port  forwarding 
Another form of NAT  is  port forwarding. With port forwarding, traffic to a  single port is  forwarded 
either to a  different port on the same machine, or to a  port on a  different machine. This 
mechanism is typically used to "hide" a  server  behind another machine, or to provide access to a 
service on  an alternate port. 

Important 
When a  port forward  is configured to forward packets to a  different machine, any 
replies from that machine will normally be  sent directly to  the original client  from that
machine. Since this will result in an invalid connection on most configurations,  the 
machine that is forwarded to will have  to be masqueraded through the firewa ll that 
performed the port  forwarding. 
A common configuration is to forward a  port  from the firewall machine to a machine 
that is already masqueraded behind the firewall. 

As an example,  the following command will forwa rd  incoming con nections on port 513/TCP on 
the fi rewa ll to port 132/TCP on the machine with the IP address 192  . 168 . 0.  254 for clients 
from the public zone:

firewall-cmd --permanent --zone=public  --add-forward-port=port=513:proto=tcp:toport=132:toaddr=192.168.0.254

To gain more  control over port forwarding rules, the following syntax can be  used with rich rules: 
forward-port  port=<PORTNUM>  protocol=tcp|udp  [to-port=<PORTNUM>]  [to-addr=<ADDRESS>]

SELinux port  labeling 
SELinux does more  than  just  file and process  labeling. Network traffic is  also tightly enforced 
by the SELinux policy. One of the methods that SELinux uses for controlling network traffic 
is labeling network ports; for example,  in the targeted policy,  port 22/TCP has the  label 
ssh_port_t associated with it. 
Whenever a  process wants to  listen on a port, SELinux will check to see if the label associated 
with that process (the domain) is allowed to bind that port  label. This can stop a  rogue service 
from taking over ports otherwise used by other (legitimate) network services.

[root@serverx  -]#  semanage  port  -1 
http_cache_port_t 	tcp  8080,  8118 ,  8123 ,  10001 - 10010 
http_cache_port_t 	udp  3130 
http_port_t 		tcp  80 ,  81,  443 ,  488 ,  8008 ,  8009 ,  8443 ,  9000 
......

For example,  to  allow a  gopher service to  listen on port 71/TCP:

semanage  port -a -t  gopher_port_t  -p  tcp  71

To remove the label:

semanage  port  -d -t  gopher_port_t  -p  tcp  71 
